{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dbamman/nlp20/blob/master/HW_4/HW_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5KTrLJKD-l7"
   },
   "source": [
    "# Homework 4: Neural Sequence Labeling\n",
    "\n",
    "**Due March 4, 2020 at 11:59PM**\n",
    "\n",
    "\n",
    "In this homework, you will be implementing, training, and evaluating an LSTM for part-of-speech tagging using the PyTorch library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DFgEH9BWEqPV"
   },
   "source": [
    "**Before beginning, please switch your Colab session to a GPU runtime** \n",
    "\n",
    "Go to Runtime > Change runtime type > Hardware accelerator > GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xALTxzWIEkkE"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kk8GRWIkbrU1"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t87Vex5_b5MI",
    "outputId": "10c8da23-d436-4054-8c94-a59c1dda1df9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "# if this cell prints \"Running on cpu\", you must switch runtime environments\n",
    "# go to Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3nDS9MwFCVf"
   },
   "source": [
    "### Download & Load Pretrained Embeddings\n",
    "\n",
    "In this assignment, we will be using GloVe pretrained word embeddings. You can read more about GloVe here: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "**Note**: this section will take *several minutes*, since the embedding files are large. Files in Colab may be cached between sessions, so you may or may not need to redownload the files each time you reconnect. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "csWld6ckFNL1",
    "outputId": "ee9d048c-2e41-40a3-e1c6-d3b6e28eaf67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘glove.6B.zip’ already there; not retrieving.\n",
      "\n",
      "Archive:  glove.6B.zip\n"
     ]
    }
   ],
   "source": [
    "# download pretrained word embeddings\n",
    "!wget -nc http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -n glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iiuJ5eylL0A0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"def read_embeddings(filename, vocab_size=10000):\\n    \\\"\\\"\\\"\\n    Utility function, loads in the `vocab_size` most common embeddings from `filename`\\n\\n    Arguments:\\n    - filename:     path to file\\n                  automatically infers correct embedding dimension from filename\\n    - vocab_size:   maximum number of embeddings to load\\n\\n    Returns \\n    - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\\n    - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\\n    \\\"\\\"\\\"\\n\\n    # get the embedding size from the first embedding\\n    with open(filename, encoding=\\\"utf-8\\\") as file:\\n        word_embedding_dim = len(file.readline().split(\\\" \\\")) - 1\\n\\n    vocab = {}\\n\\n    embeddings = np.zeros((vocab_size, word_embedding_dim))\\n\\n    with open(filename, encoding=\\\"utf-8\\\") as file:\\n        for idx, line in enumerate(file):\\n\\n            if idx + 2 >= vocab_size:\\n                break\\n\\n            cols = line.rstrip().split(\\\" \\\")\\n            val = np.array(cols[1:])\\n            word = cols[0]\\n            embeddings[idx + 2] = val\\n            vocab[word] = idx + 2\\n\\n    # a FloatTensor is a multidimensional matrix\\n    # that contains 32-bit floats in every entry\\n    # https://pytorch.org/docs/stable/tensors.html\\n    return torch.FloatTensor(embeddings), vocab\";\n",
       "                var nbb_formatted_code = \"def read_embeddings(filename, vocab_size=10000):\\n    \\\"\\\"\\\"\\n    Utility function, loads in the `vocab_size` most common embeddings from `filename`\\n\\n    Arguments:\\n    - filename:     path to file\\n                  automatically infers correct embedding dimension from filename\\n    - vocab_size:   maximum number of embeddings to load\\n\\n    Returns \\n    - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\\n    - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\\n    \\\"\\\"\\\"\\n\\n    # get the embedding size from the first embedding\\n    with open(filename, encoding=\\\"utf-8\\\") as file:\\n        word_embedding_dim = len(file.readline().split(\\\" \\\")) - 1\\n\\n    vocab = {}\\n\\n    embeddings = np.zeros((vocab_size, word_embedding_dim))\\n\\n    with open(filename, encoding=\\\"utf-8\\\") as file:\\n        for idx, line in enumerate(file):\\n\\n            if idx + 2 >= vocab_size:\\n                break\\n\\n            cols = line.rstrip().split(\\\" \\\")\\n            val = np.array(cols[1:])\\n            word = cols[0]\\n            embeddings[idx + 2] = val\\n            vocab[word] = idx + 2\\n\\n    # a FloatTensor is a multidimensional matrix\\n    # that contains 32-bit floats in every entry\\n    # https://pytorch.org/docs/stable/tensors.html\\n    return torch.FloatTensor(embeddings), vocab\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_embeddings(filename, vocab_size=10000):\n",
    "    \"\"\"\n",
    "    Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
    "\n",
    "    Arguments:\n",
    "    - filename:     path to file\n",
    "                  automatically infers correct embedding dimension from filename\n",
    "    - vocab_size:   maximum number of embeddings to load\n",
    "\n",
    "    Returns \n",
    "    - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
    "    - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # get the embedding size from the first embedding\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
    "\n",
    "    vocab = {}\n",
    "\n",
    "    embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
    "\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "\n",
    "            if idx + 2 >= vocab_size:\n",
    "                break\n",
    "\n",
    "            cols = line.rstrip().split(\" \")\n",
    "            val = np.array(cols[1:])\n",
    "            word = cols[0]\n",
    "            embeddings[idx + 2] = val\n",
    "            vocab[word] = idx + 2\n",
    "\n",
    "    # a FloatTensor is a multidimensional matrix\n",
    "    # that contains 32-bit floats in every entry\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    return torch.FloatTensor(embeddings), vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ah3clY7lHNZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Running the cell below lists all the files in the current directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "4D__oK6mQ6hs",
    "outputId": "4296ab94-c740-4928-db00-794f960dabf4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3.0G\r\n",
      "-rw-r--r-- 1 root root 457K Feb 29 18:06 HW_4.html\r\n",
      "-rw-r--r-- 1 root root  47K Feb 29 18:34 HW_4.ipynb\r\n",
      "lrwxrwxrwx 1 root root    9 Feb 29 18:12 datasets -> /datasets\r\n",
      "-rw-r--r-- 1 root root 332M Feb 29 18:07 glove.6B.100d.txt\r\n",
      "-rw-r--r-- 1 root root 662M Feb 29 18:08 glove.6B.200d.txt\r\n",
      "-rw-r--r-- 1 root root 990M Feb 29 18:10 glove.6B.300d.txt\r\n",
      "-rw-r--r-- 1 root root 164M Feb 29 18:10 glove.6B.50d.txt\r\n",
      "-rw-r--r-- 1 root root 823M Feb 29 18:12 glove.6B.zip\r\n",
      "-rw-r--r-- 1 root root 209K Feb 29 18:12 pos.dev\r\n",
      "-rw-r--r-- 1 root root  319 Feb 29 18:12 pos.tagset\r\n",
      "-rw-r--r-- 1 root root 128K Feb 29 18:12 pos.test\r\n",
      "-rw-r--r-- 1 root root 1.7M Feb 29 18:12 pos.train\r\n",
      "-rw-r--r-- 1 root root  70K Feb 29 18:12 predictions.txt\r\n",
      "lrwxrwxrwx 1 root root    8 Feb 29 18:12 storage -> /storage\r\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"!ls -lh\";\n",
       "                var nbb_formatted_code = \"!ls -lh\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aoSWrCwZllg6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should see several embedding files, which are all formatted as\n",
    "\n",
    "```\n",
    "glove.6B.<emb_dim>d.txt\n",
    "```\n",
    "\n",
    "Each `txt` file contains `emb_dim` dimensional embeddings for 400,000 unique, uncased words. The script below loads the `vocab_size` most common words from the embedding file into a matrix we can give to our model. All other words will later be mapped to the `UNKNOWN` embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xL8WuEZoOFbs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 111;\n",
       "                var nbb_unformatted_code = \"# this loads the 10,000 most common word 50-dimensional embeddings\\nvocab_size = 10000\\nembeddings, vocab = read_embeddings(\\\"glove.6B.50d.txt\\\", vocab_size)\";\n",
       "                var nbb_formatted_code = \"# this loads the 10,000 most common word 50-dimensional embeddings\\nvocab_size = 10000\\nembeddings, vocab = read_embeddings(\\\"glove.6B.50d.txt\\\", vocab_size)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this loads the 10,000 most common word 50-dimensional embeddings\n",
    "vocab_size = 10000\n",
    "embeddings, vocab = read_embeddings(\"glove.6B.50d.txt\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "py9AStUOJnPB"
   },
   "source": [
    "## Part 1: Batching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5jWl-z8w_5t",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement the `get_batches` function in the `Dataset` class below. \n",
    "\n",
    "**Please make sure that**\n",
    "\n",
    "*   Your implementation is self-contained. That is, all helper functions and variables are defined within `get_batches`.\n",
    "*   Your implementation can handle variable batch sizes. You may not assume that the value with always be 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2KWyqb2HcLop"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"class Dataset:\\n    def __init__(self, filename, is_labeled):\\n        self.is_labeled = is_labeled\\n        # if the file is not labeled, the Dataset has no tags (see read_data)\\n        if is_labeled:\\n            self.sentences, self.tags = self.read_data(filename, is_labeled)\\n        else:\\n            self.sentences = self.read_data(filename, is_labeled)\\n            self.tags = None\\n\\n    def read_data(self, filename, is_labeled):\\n        \\\"\\\"\\\"\\n        Utility function, loads text file into a list of sentence and tag strings\\n\\n        Arguments:\\n        - filename:     path to file\\n        - is_labeled:   whether the file contains tags for each word or not\\n            > if True, we assume each line is formatted as \\\"<word>\\\\t<tag>\\\\n\\\"\\n            > if False, we assume each line is formatted as \\\"<word>\\\\n\\\"\\n\\n        Returns:\\n        - sentences:    a list of sentences, where each sentence is a list \\n                        words (strings)\\n\\n        if is_labeled=True, also returns\\n        - tags:         a list of tags for each sentence, where tags[i] contains\\n                        a list of tags (strings) that correspond to the words in \\n                        sentences[i]\\n        \\\"\\\"\\\"\\n        sentences = []\\n        tags = []\\n\\n        current_sentence = []\\n        current_tags = []\\n\\n        with open(filename, encoding=\\\"utf8\\\") as f:\\n            # iterate over the lines in the file\\n            for line in f:\\n                if len(line) == 0:\\n                    continue\\n                if line == \\\"\\\\n\\\":\\n                    if len(current_sentence) != 0:\\n                        sentences.append(current_sentence)\\n                        tags.append(current_tags)\\n\\n                    current_sentence = []\\n                    current_tags = []\\n                else:\\n                    if is_labeled:\\n                        columns = line.rstrip().split(\\\"\\\\t\\\")\\n                        word = columns[0].lower()\\n                        tag = columns[1]\\n\\n                        current_sentence.append(word)\\n                        current_tags.append(tag)\\n                    else:\\n                        column = line.rstrip().split(\\\"\\\\t\\\")\\n                        word = column[0].lower()\\n                        current_sentence.append(word)\\n\\n            if is_labeled:\\n                return sentences, tags\\n            else:\\n                return sentences\\n\\n    def get_batches(self, batch_size, vocab, tagset):\\n        \\\"\\\"\\\"\\n\\n        Batches the data into mini-batches of size `batch_size`\\n\\n        Arguments:\\n        - batch_size:       the desired output batch size\\n        - vocab:            a dictionary mapping word strings to indices\\n        - tagset:           a dictionary mapping tag strings to indices\\n\\n        Outputs:\\n\\n        if is_labeled=True:\\n        - batched_word_indices:     a list of matrices of dimension (batch_size x max_seq_len)\\n        - batched_tag_indices:      a list of matrices of dimension (batch_size x max_seq_len)\\n        - batched_lengths:          a list of arrays of length (batch_size)\\n\\n        if is_labeled=False:\\n        - batched_word_indices:     a list of matrices of dimension (batch_size x max_seq_len)\\n        - batched_lengths:          a list of arrays of length (batch_size)\\n\\n\\n        Description: \\n\\n        This function partitions the data into batches of size batch_size. If the number\\n        of sentences in the document is not an even multiple of batch_size, the final batch\\n        will contain the remaining elements. For example, if there are 82 sentences in the \\n        dataset and batch_size=32, we return a list containing two batches of size 32 \\n        and one final batch of size 18.\\n\\n        batched_word_indices[b] is a (batch_size x max_seq_len) matrix of integers, \\n        containing index representations for sentences in the b-th batch in the document. \\n        The `vocab` dictionary provides the correct mapping from word strings to indices. \\n        If a word is not in the vocabulary, it gets mapped to UNKNOWN_INDEX (1).\\n        `max_seq_len` is the maximum sentence length among the sentences in the current batch, \\n        which will vary between different batches. All sentences shorter than max_seq_len \\n        should be padded on the right with PAD_INDEX (0).\\n\\n        If the document is labeled, we also batch the document's tags. Analogous to \\n        batched_word_indices, batched_tag_indices[b] contains the index representation\\n        for the tags corresponding to the sentences in the b-th batch  in the document. \\n        The `tagset` dictionary provides the correct mapping from tag strings to indicies. \\n        All tag lists shorter than `max_seq_len` are padded with IGNORE_TAG_INDEX (-100).\\n\\n        batched_lengths[b] is a vector of length (batch_size). batched_lengths[b][i] \\n        contains the original sentence length *before* padding for the i-th sentence\\n        in the currrent batch. \\n\\n        \\\"\\\"\\\"\\n        PAD_INDEX = 0  # reserved for padding words\\n        UNKNOWN_INDEX = 1  # reserved for unknown words\\n        IGNORE_TAG_INDEX = -100  # reserved for padding tags\\n\\n        # randomly shuffle the data\\n        np.random.seed(159)  # DON'T CHANGE THIS\\n        shuffle = np.random.permutation(range(len(self.sentences)))\\n\\n        sentences = [self.sentences[i] for i in shuffle]\\n        if self.is_labeled:\\n            tags = [self.tags[i] for i in shuffle]\\n        else:\\n            tags = None\\n\\n        batched_word_indices = []\\n        batched_tag_indices = []\\n        batched_lengths = []\\n\\n        # prepare sentences and lengths\\n        for i in range(0, len(sentences), batch_size):\\n\\n            sent_chunk = sentences[i : i + batch_size]\\n            max_seq_len = max(len(sent) for sent in sent_chunk)\\n            word_matrix = []\\n\\n            # prepare lengths\\n            lengths = [len(sent) for sent in sent_chunk]\\n            batched_lengths.append(np.array(lengths))\\n\\n            # prepare sentences\\n            for sent in sent_chunk:\\n                sent_word_indices = [PAD_INDEX] * max_seq_len\\n\\n                for idx, word in enumerate(sent):\\n                    sent_word_indices[idx] = vocab.get(word, UNKNOWN_INDEX)\\n\\n                word_matrix.append(sent_word_indices)\\n\\n            batched_word_indices.append(np.array(word_matrix))\\n\\n        # prepare tags\\n        if tags:\\n            for i in range(0, len(tags), batch_size):\\n                chunk = tags[i : i + batch_size]\\n                max_seq_len = max(len(x) for x in chunk)\\n                tag_matrix = []\\n\\n                for sentence_tags in chunk:\\n                    tag_indices = [IGNORE_TAG_INDEX] * max_seq_len\\n\\n                    for idx, tag in enumerate(sentence_tags):\\n                        tag_indices[idx] = tagset[tag]\\n\\n                    tag_matrix.append(tag_indices)\\n\\n                batched_tag_indices.append(np.array(tag_matrix))\\n\\n        #############################\\n        #       DO NOT MODIFY       #\\n        #############################\\n        if self.is_labeled:\\n            return batched_word_indices, batched_tag_indices, batched_lengths\\n        else:\\n            return batched_word_indices, batched_lengths\";\n",
       "                var nbb_formatted_code = \"class Dataset:\\n    def __init__(self, filename, is_labeled):\\n        self.is_labeled = is_labeled\\n        # if the file is not labeled, the Dataset has no tags (see read_data)\\n        if is_labeled:\\n            self.sentences, self.tags = self.read_data(filename, is_labeled)\\n        else:\\n            self.sentences = self.read_data(filename, is_labeled)\\n            self.tags = None\\n\\n    def read_data(self, filename, is_labeled):\\n        \\\"\\\"\\\"\\n        Utility function, loads text file into a list of sentence and tag strings\\n\\n        Arguments:\\n        - filename:     path to file\\n        - is_labeled:   whether the file contains tags for each word or not\\n            > if True, we assume each line is formatted as \\\"<word>\\\\t<tag>\\\\n\\\"\\n            > if False, we assume each line is formatted as \\\"<word>\\\\n\\\"\\n\\n        Returns:\\n        - sentences:    a list of sentences, where each sentence is a list \\n                        words (strings)\\n\\n        if is_labeled=True, also returns\\n        - tags:         a list of tags for each sentence, where tags[i] contains\\n                        a list of tags (strings) that correspond to the words in \\n                        sentences[i]\\n        \\\"\\\"\\\"\\n        sentences = []\\n        tags = []\\n\\n        current_sentence = []\\n        current_tags = []\\n\\n        with open(filename, encoding=\\\"utf8\\\") as f:\\n            # iterate over the lines in the file\\n            for line in f:\\n                if len(line) == 0:\\n                    continue\\n                if line == \\\"\\\\n\\\":\\n                    if len(current_sentence) != 0:\\n                        sentences.append(current_sentence)\\n                        tags.append(current_tags)\\n\\n                    current_sentence = []\\n                    current_tags = []\\n                else:\\n                    if is_labeled:\\n                        columns = line.rstrip().split(\\\"\\\\t\\\")\\n                        word = columns[0].lower()\\n                        tag = columns[1]\\n\\n                        current_sentence.append(word)\\n                        current_tags.append(tag)\\n                    else:\\n                        column = line.rstrip().split(\\\"\\\\t\\\")\\n                        word = column[0].lower()\\n                        current_sentence.append(word)\\n\\n            if is_labeled:\\n                return sentences, tags\\n            else:\\n                return sentences\\n\\n    def get_batches(self, batch_size, vocab, tagset):\\n        \\\"\\\"\\\"\\n\\n        Batches the data into mini-batches of size `batch_size`\\n\\n        Arguments:\\n        - batch_size:       the desired output batch size\\n        - vocab:            a dictionary mapping word strings to indices\\n        - tagset:           a dictionary mapping tag strings to indices\\n\\n        Outputs:\\n\\n        if is_labeled=True:\\n        - batched_word_indices:     a list of matrices of dimension (batch_size x max_seq_len)\\n        - batched_tag_indices:      a list of matrices of dimension (batch_size x max_seq_len)\\n        - batched_lengths:          a list of arrays of length (batch_size)\\n\\n        if is_labeled=False:\\n        - batched_word_indices:     a list of matrices of dimension (batch_size x max_seq_len)\\n        - batched_lengths:          a list of arrays of length (batch_size)\\n\\n\\n        Description: \\n\\n        This function partitions the data into batches of size batch_size. If the number\\n        of sentences in the document is not an even multiple of batch_size, the final batch\\n        will contain the remaining elements. For example, if there are 82 sentences in the \\n        dataset and batch_size=32, we return a list containing two batches of size 32 \\n        and one final batch of size 18.\\n\\n        batched_word_indices[b] is a (batch_size x max_seq_len) matrix of integers, \\n        containing index representations for sentences in the b-th batch in the document. \\n        The `vocab` dictionary provides the correct mapping from word strings to indices. \\n        If a word is not in the vocabulary, it gets mapped to UNKNOWN_INDEX (1).\\n        `max_seq_len` is the maximum sentence length among the sentences in the current batch, \\n        which will vary between different batches. All sentences shorter than max_seq_len \\n        should be padded on the right with PAD_INDEX (0).\\n\\n        If the document is labeled, we also batch the document's tags. Analogous to \\n        batched_word_indices, batched_tag_indices[b] contains the index representation\\n        for the tags corresponding to the sentences in the b-th batch  in the document. \\n        The `tagset` dictionary provides the correct mapping from tag strings to indicies. \\n        All tag lists shorter than `max_seq_len` are padded with IGNORE_TAG_INDEX (-100).\\n\\n        batched_lengths[b] is a vector of length (batch_size). batched_lengths[b][i] \\n        contains the original sentence length *before* padding for the i-th sentence\\n        in the currrent batch. \\n\\n        \\\"\\\"\\\"\\n        PAD_INDEX = 0  # reserved for padding words\\n        UNKNOWN_INDEX = 1  # reserved for unknown words\\n        IGNORE_TAG_INDEX = -100  # reserved for padding tags\\n\\n        # randomly shuffle the data\\n        np.random.seed(159)  # DON'T CHANGE THIS\\n        shuffle = np.random.permutation(range(len(self.sentences)))\\n\\n        sentences = [self.sentences[i] for i in shuffle]\\n        if self.is_labeled:\\n            tags = [self.tags[i] for i in shuffle]\\n        else:\\n            tags = None\\n\\n        batched_word_indices = []\\n        batched_tag_indices = []\\n        batched_lengths = []\\n\\n        # prepare sentences and lengths\\n        for i in range(0, len(sentences), batch_size):\\n\\n            sent_chunk = sentences[i : i + batch_size]\\n            max_seq_len = max(len(sent) for sent in sent_chunk)\\n            word_matrix = []\\n\\n            # prepare lengths\\n            lengths = [len(sent) for sent in sent_chunk]\\n            batched_lengths.append(np.array(lengths))\\n\\n            # prepare sentences\\n            for sent in sent_chunk:\\n                sent_word_indices = [PAD_INDEX] * max_seq_len\\n\\n                for idx, word in enumerate(sent):\\n                    sent_word_indices[idx] = vocab.get(word, UNKNOWN_INDEX)\\n\\n                word_matrix.append(sent_word_indices)\\n\\n            batched_word_indices.append(np.array(word_matrix))\\n\\n        # prepare tags\\n        if tags:\\n            for i in range(0, len(tags), batch_size):\\n                chunk = tags[i : i + batch_size]\\n                max_seq_len = max(len(x) for x in chunk)\\n                tag_matrix = []\\n\\n                for sentence_tags in chunk:\\n                    tag_indices = [IGNORE_TAG_INDEX] * max_seq_len\\n\\n                    for idx, tag in enumerate(sentence_tags):\\n                        tag_indices[idx] = tagset[tag]\\n\\n                    tag_matrix.append(tag_indices)\\n\\n                batched_tag_indices.append(np.array(tag_matrix))\\n\\n        #############################\\n        #       DO NOT MODIFY       #\\n        #############################\\n        if self.is_labeled:\\n            return batched_word_indices, batched_tag_indices, batched_lengths\\n        else:\\n            return batched_word_indices, batched_lengths\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, filename, is_labeled):\n",
    "        self.is_labeled = is_labeled\n",
    "        # if the file is not labeled, the Dataset has no tags (see read_data)\n",
    "        if is_labeled:\n",
    "            self.sentences, self.tags = self.read_data(filename, is_labeled)\n",
    "        else:\n",
    "            self.sentences = self.read_data(filename, is_labeled)\n",
    "            self.tags = None\n",
    "\n",
    "    def read_data(self, filename, is_labeled):\n",
    "        \"\"\"\n",
    "        Utility function, loads text file into a list of sentence and tag strings\n",
    "\n",
    "        Arguments:\n",
    "        - filename:     path to file\n",
    "        - is_labeled:   whether the file contains tags for each word or not\n",
    "            > if True, we assume each line is formatted as \"<word>\\t<tag>\\n\"\n",
    "            > if False, we assume each line is formatted as \"<word>\\n\"\n",
    "\n",
    "        Returns:\n",
    "        - sentences:    a list of sentences, where each sentence is a list \n",
    "                        words (strings)\n",
    "\n",
    "        if is_labeled=True, also returns\n",
    "        - tags:         a list of tags for each sentence, where tags[i] contains\n",
    "                        a list of tags (strings) that correspond to the words in \n",
    "                        sentences[i]\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        tags = []\n",
    "\n",
    "        current_sentence = []\n",
    "        current_tags = []\n",
    "\n",
    "        with open(filename, encoding=\"utf8\") as f:\n",
    "            # iterate over the lines in the file\n",
    "            for line in f:\n",
    "                if len(line) == 0:\n",
    "                    continue\n",
    "                if line == \"\\n\":\n",
    "                    if len(current_sentence) != 0:\n",
    "                        sentences.append(current_sentence)\n",
    "                        tags.append(current_tags)\n",
    "\n",
    "                    current_sentence = []\n",
    "                    current_tags = []\n",
    "                else:\n",
    "                    if is_labeled:\n",
    "                        columns = line.rstrip().split(\"\\t\")\n",
    "                        word = columns[0].lower()\n",
    "                        tag = columns[1]\n",
    "\n",
    "                        current_sentence.append(word)\n",
    "                        current_tags.append(tag)\n",
    "                    else:\n",
    "                        column = line.rstrip().split(\"\\t\")\n",
    "                        word = column[0].lower()\n",
    "                        current_sentence.append(word)\n",
    "\n",
    "            if is_labeled:\n",
    "                return sentences, tags\n",
    "            else:\n",
    "                return sentences\n",
    "\n",
    "    def get_batches(self, batch_size, vocab, tagset):\n",
    "        \"\"\"\n",
    "\n",
    "        Batches the data into mini-batches of size `batch_size`\n",
    "\n",
    "        Arguments:\n",
    "        - batch_size:       the desired output batch size\n",
    "        - vocab:            a dictionary mapping word strings to indices\n",
    "        - tagset:           a dictionary mapping tag strings to indices\n",
    "\n",
    "        Outputs:\n",
    "\n",
    "        if is_labeled=True:\n",
    "        - batched_word_indices:     a list of matrices of dimension (batch_size x max_seq_len)\n",
    "        - batched_tag_indices:      a list of matrices of dimension (batch_size x max_seq_len)\n",
    "        - batched_lengths:          a list of arrays of length (batch_size)\n",
    "\n",
    "        if is_labeled=False:\n",
    "        - batched_word_indices:     a list of matrices of dimension (batch_size x max_seq_len)\n",
    "        - batched_lengths:          a list of arrays of length (batch_size)\n",
    "\n",
    "\n",
    "        Description: \n",
    "\n",
    "        This function partitions the data into batches of size batch_size. If the number\n",
    "        of sentences in the document is not an even multiple of batch_size, the final batch\n",
    "        will contain the remaining elements. For example, if there are 82 sentences in the \n",
    "        dataset and batch_size=32, we return a list containing two batches of size 32 \n",
    "        and one final batch of size 18.\n",
    "\n",
    "        batched_word_indices[b] is a (batch_size x max_seq_len) matrix of integers, \n",
    "        containing index representations for sentences in the b-th batch in the document. \n",
    "        The `vocab` dictionary provides the correct mapping from word strings to indices. \n",
    "        If a word is not in the vocabulary, it gets mapped to UNKNOWN_INDEX (1).\n",
    "        `max_seq_len` is the maximum sentence length among the sentences in the current batch, \n",
    "        which will vary between different batches. All sentences shorter than max_seq_len \n",
    "        should be padded on the right with PAD_INDEX (0).\n",
    "\n",
    "        If the document is labeled, we also batch the document's tags. Analogous to \n",
    "        batched_word_indices, batched_tag_indices[b] contains the index representation\n",
    "        for the tags corresponding to the sentences in the b-th batch  in the document. \n",
    "        The `tagset` dictionary provides the correct mapping from tag strings to indicies. \n",
    "        All tag lists shorter than `max_seq_len` are padded with IGNORE_TAG_INDEX (-100).\n",
    "\n",
    "        batched_lengths[b] is a vector of length (batch_size). batched_lengths[b][i] \n",
    "        contains the original sentence length *before* padding for the i-th sentence\n",
    "        in the currrent batch. \n",
    "\n",
    "        \"\"\"\n",
    "        PAD_INDEX = 0  # reserved for padding words\n",
    "        UNKNOWN_INDEX = 1  # reserved for unknown words\n",
    "        IGNORE_TAG_INDEX = -100  # reserved for padding tags\n",
    "\n",
    "        # randomly shuffle the data\n",
    "        np.random.seed(159)  # DON'T CHANGE THIS\n",
    "        shuffle = np.random.permutation(range(len(self.sentences)))\n",
    "\n",
    "        sentences = [self.sentences[i] for i in shuffle]\n",
    "        if self.is_labeled:\n",
    "            tags = [self.tags[i] for i in shuffle]\n",
    "        else:\n",
    "            tags = None\n",
    "\n",
    "        batched_word_indices = []\n",
    "        batched_tag_indices = []\n",
    "        batched_lengths = []\n",
    "\n",
    "        # prepare sentences and lengths\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "\n",
    "            sent_chunk = sentences[i : i + batch_size]\n",
    "            max_seq_len = max(len(sent) for sent in sent_chunk)\n",
    "            word_matrix = []\n",
    "\n",
    "            # prepare lengths\n",
    "            lengths = [len(sent) for sent in sent_chunk]\n",
    "            batched_lengths.append(np.array(lengths))\n",
    "\n",
    "            # prepare sentences\n",
    "            for sent in sent_chunk:\n",
    "                sent_word_indices = [PAD_INDEX] * max_seq_len\n",
    "\n",
    "                for idx, word in enumerate(sent):\n",
    "                    sent_word_indices[idx] = vocab.get(word, UNKNOWN_INDEX)\n",
    "\n",
    "                word_matrix.append(sent_word_indices)\n",
    "\n",
    "            batched_word_indices.append(np.array(word_matrix))\n",
    "\n",
    "        # prepare tags\n",
    "        if tags:\n",
    "            for i in range(0, len(tags), batch_size):\n",
    "                chunk = tags[i : i + batch_size]\n",
    "                max_seq_len = max(len(x) for x in chunk)\n",
    "                tag_matrix = []\n",
    "\n",
    "                for sentence_tags in chunk:\n",
    "                    tag_indices = [IGNORE_TAG_INDEX] * max_seq_len\n",
    "\n",
    "                    for idx, tag in enumerate(sentence_tags):\n",
    "                        tag_indices[idx] = tagset[tag]\n",
    "\n",
    "                    tag_matrix.append(tag_indices)\n",
    "\n",
    "                batched_tag_indices.append(np.array(tag_matrix))\n",
    "\n",
    "        #############################\n",
    "        #       DO NOT MODIFY       #\n",
    "        #############################\n",
    "        if self.is_labeled:\n",
    "            return batched_word_indices, batched_tag_indices, batched_lengths\n",
    "        else:\n",
    "            return batched_word_indices, batched_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ePEcb46_zGS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"def read_tagset(tag_file):\\n    \\\"\\\"\\\"\\n    Utility function, loads tag file into a dictionary from tag string to tag index\\n\\n    Arguments:\\n    - tag_file:   file location of the tagset\\n\\n    Outputs:\\n    - tagset:     a dictionary mapping tag strings (e.g. \\\"VB\\\") to a unique index\\n    \\\"\\\"\\\"\\n    tagset = {}\\n    with open(tag_file, encoding=\\\"utf8\\\") as f:\\n        for line in f:\\n            columns = line.rstrip().split(\\\"\\\\t\\\")\\n            tag = columns[0]\\n            tag_id = int(columns[1])\\n            tagset[tag] = tag_id\\n\\n    return tagset\";\n",
       "                var nbb_formatted_code = \"def read_tagset(tag_file):\\n    \\\"\\\"\\\"\\n    Utility function, loads tag file into a dictionary from tag string to tag index\\n\\n    Arguments:\\n    - tag_file:   file location of the tagset\\n\\n    Outputs:\\n    - tagset:     a dictionary mapping tag strings (e.g. \\\"VB\\\") to a unique index\\n    \\\"\\\"\\\"\\n    tagset = {}\\n    with open(tag_file, encoding=\\\"utf8\\\") as f:\\n        for line in f:\\n            columns = line.rstrip().split(\\\"\\\\t\\\")\\n            tag = columns[0]\\n            tag_id = int(columns[1])\\n            tagset[tag] = tag_id\\n\\n    return tagset\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_tagset(tag_file):\n",
    "    \"\"\"\n",
    "    Utility function, loads tag file into a dictionary from tag string to tag index\n",
    "\n",
    "    Arguments:\n",
    "    - tag_file:   file location of the tagset\n",
    "\n",
    "    Outputs:\n",
    "    - tagset:     a dictionary mapping tag strings (e.g. \"VB\") to a unique index\n",
    "    \"\"\"\n",
    "    tagset = {}\n",
    "    with open(tag_file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            columns = line.rstrip().split(\"\\t\")\n",
    "            tag = columns[0]\n",
    "            tag_id = int(columns[1])\n",
    "            tagset[tag] = tag_id\n",
    "\n",
    "    return tagset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8mMQIJ_vVuiz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The cells below download the data files and construct the corresponding `Dataset` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RhXTkpTqGR1H"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"%%capture\\n!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.train\\n!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.dev\\n!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.test\\n!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.tagset\";\n",
       "                var nbb_formatted_code = \"%%capture\\n!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.train\\n!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.dev\\n!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.test\\n!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.tagset\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%capture\n",
    "!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.train\n",
    "!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.dev\n",
    "!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.test\n",
    "!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtP5seIgBORT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# read the files\\ntagset = read_tagset(\\\"pos.tagset\\\")\\ntrain_dataset = Dataset(\\\"pos.train\\\", is_labeled=True)\\ndev_dataset = Dataset(\\\"pos.dev\\\", is_labeled=True)\\ntest_dataset = Dataset(\\\"pos.test\\\", is_labeled=False)\\n\\nBATCH_SIZE = 32\\n\\n# these should run without errors if implemented correctly\\ntrain_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(\\n    BATCH_SIZE, vocab, tagset\\n)\\ndev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(\\n    BATCH_SIZE, vocab, tagset\\n)\\ntest_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)\";\n",
       "                var nbb_formatted_code = \"# read the files\\ntagset = read_tagset(\\\"pos.tagset\\\")\\ntrain_dataset = Dataset(\\\"pos.train\\\", is_labeled=True)\\ndev_dataset = Dataset(\\\"pos.dev\\\", is_labeled=True)\\ntest_dataset = Dataset(\\\"pos.test\\\", is_labeled=False)\\n\\nBATCH_SIZE = 32\\n\\n# these should run without errors if implemented correctly\\ntrain_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(\\n    BATCH_SIZE, vocab, tagset\\n)\\ndev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(\\n    BATCH_SIZE, vocab, tagset\\n)\\ntest_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the files\n",
    "tagset = read_tagset(\"pos.tagset\")\n",
    "train_dataset = Dataset(\"pos.train\", is_labeled=True)\n",
    "dev_dataset = Dataset(\"pos.dev\", is_labeled=True)\n",
    "test_dataset = Dataset(\"pos.test\", is_labeled=False)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# these should run without errors if implemented correctly\n",
    "train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(\n",
    "    BATCH_SIZE, vocab, tagset\n",
    ")\n",
    "dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(\n",
    "    BATCH_SIZE, vocab, tagset\n",
    ")\n",
    "test_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-nnX7WxqDJ3"
   },
   "source": [
    "### Part 2: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oMtkCBt_wzIS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will implement utility functions that will later be used to assess our model's perfomance. \n",
    "\n",
    "**Please make sure that**\n",
    "\n",
    "*   Your implementation is self-contained. That is, keep all helper functions or variables inside of your function.\n",
    "*   Your implementation does not import any additional libraries. You will not receive credit if you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQLiM0ukG-4W"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# The accuracy function has been implemented for you\\n\\n\\ndef accuracy(true, pred):\\n    \\\"\\\"\\\"\\n    Arguments:\\n    - true:       a list of true label values (integers)\\n    - pred:       a list of predicted label values (integers)\\n\\n    Output:\\n    - accuracy:   the prediction accuracy\\n    \\\"\\\"\\\"\\n    true = np.array(true)\\n    pred = np.array(pred)\\n\\n    num_correct = sum(true == pred)\\n    num_total = len(true)\\n    return num_correct / num_total\";\n",
       "                var nbb_formatted_code = \"# The accuracy function has been implemented for you\\n\\n\\ndef accuracy(true, pred):\\n    \\\"\\\"\\\"\\n    Arguments:\\n    - true:       a list of true label values (integers)\\n    - pred:       a list of predicted label values (integers)\\n\\n    Output:\\n    - accuracy:   the prediction accuracy\\n    \\\"\\\"\\\"\\n    true = np.array(true)\\n    pred = np.array(pred)\\n\\n    num_correct = sum(true == pred)\\n    num_total = len(true)\\n    return num_correct / num_total\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The accuracy function has been implemented for you\n",
    "\n",
    "\n",
    "def accuracy(true, pred):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - true:       a list of true label values (integers)\n",
    "    - pred:       a list of predicted label values (integers)\n",
    "\n",
    "    Output:\n",
    "    - accuracy:   the prediction accuracy\n",
    "    \"\"\"\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    num_correct = sum(true == pred)\n",
    "    num_total = len(true)\n",
    "    return num_correct / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChJjUu45qFM8"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"def confusion_matrix(true, pred, num_tags):\\n    \\\"\\\"\\\"\\n    Arguments:\\n    - true:       a list of true label values (integers)\\n    - pred:       a list of predicted label values (integers)\\n    - num_tags:   the number of possible tags\\n                true and pred will both contain integers between\\n                0 and num_tags - 1 (inclusive)\\n\\n    Output: \\n    - confusion_matrix:   a (num_tags x num_tags) matrix of integers\\n\\n    confusion_matrix[i][j] = # predictions where true label\\n    was i and predicted label was j\\n\\n    \\\"\\\"\\\"\\n\\n    confusion_matrix = np.zeros((num_tags, num_tags))\\n    true = np.array(true)\\n    pred = np.array(pred)\\n\\n    for i in range(len(true)):\\n        confusion_matrix[true[i]][pred[i]] += 1\\n\\n    return confusion_matrix\";\n",
       "                var nbb_formatted_code = \"def confusion_matrix(true, pred, num_tags):\\n    \\\"\\\"\\\"\\n    Arguments:\\n    - true:       a list of true label values (integers)\\n    - pred:       a list of predicted label values (integers)\\n    - num_tags:   the number of possible tags\\n                true and pred will both contain integers between\\n                0 and num_tags - 1 (inclusive)\\n\\n    Output: \\n    - confusion_matrix:   a (num_tags x num_tags) matrix of integers\\n\\n    confusion_matrix[i][j] = # predictions where true label\\n    was i and predicted label was j\\n\\n    \\\"\\\"\\\"\\n\\n    confusion_matrix = np.zeros((num_tags, num_tags))\\n    true = np.array(true)\\n    pred = np.array(pred)\\n\\n    for i in range(len(true)):\\n        confusion_matrix[true[i]][pred[i]] += 1\\n\\n    return confusion_matrix\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def confusion_matrix(true, pred, num_tags):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - true:       a list of true label values (integers)\n",
    "    - pred:       a list of predicted label values (integers)\n",
    "    - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "    Output: \n",
    "    - confusion_matrix:   a (num_tags x num_tags) matrix of integers\n",
    "\n",
    "    confusion_matrix[i][j] = # predictions where true label\n",
    "    was i and predicted label was j\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    confusion_matrix = np.zeros((num_tags, num_tags))\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    for i in range(len(true)):\n",
    "        confusion_matrix[true[i]][pred[i]] += 1\n",
    "\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Hdj6QSaBV9S"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"def precision(true, pred, num_tags):\\n    \\\"\\\"\\\"\\n  Arguments:\\n  - true:       a list of true label values (integers)\\n  - pred:       a list of predicted label values (integers)\\n  - num_tags:   the number of possible tags\\n                true and pred will both contain integers between\\n                0 and num_tags - 1 (inclusive)\\n\\n  Output: \\n  - precision:  an array of length num_tags, where precision[i]\\n                gives the precision of class i\\n\\n  Hints:  the confusion matrix may be useful\\n          be careful about zero division\\n  \\\"\\\"\\\"\\n\\n    precision = np.zeros(num_tags)\\n\\n    cm = confusion_matrix(true, pred, num_tags)\\n\\n    for i in range(num_tags):\\n        tp = cm[i][i]\\n        tp_fp = np.sum(cm[:, i])\\n        precision[i] = tp / tp_fp if tp_fp > 0 else 0\\n\\n    return precision\";\n",
       "                var nbb_formatted_code = \"def precision(true, pred, num_tags):\\n    \\\"\\\"\\\"\\n  Arguments:\\n  - true:       a list of true label values (integers)\\n  - pred:       a list of predicted label values (integers)\\n  - num_tags:   the number of possible tags\\n                true and pred will both contain integers between\\n                0 and num_tags - 1 (inclusive)\\n\\n  Output: \\n  - precision:  an array of length num_tags, where precision[i]\\n                gives the precision of class i\\n\\n  Hints:  the confusion matrix may be useful\\n          be careful about zero division\\n  \\\"\\\"\\\"\\n\\n    precision = np.zeros(num_tags)\\n\\n    cm = confusion_matrix(true, pred, num_tags)\\n\\n    for i in range(num_tags):\\n        tp = cm[i][i]\\n        tp_fp = np.sum(cm[:, i])\\n        precision[i] = tp / tp_fp if tp_fp > 0 else 0\\n\\n    return precision\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def precision(true, pred, num_tags):\n",
    "    \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "  - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "  Output: \n",
    "  - precision:  an array of length num_tags, where precision[i]\n",
    "                gives the precision of class i\n",
    "\n",
    "  Hints:  the confusion matrix may be useful\n",
    "          be careful about zero division\n",
    "  \"\"\"\n",
    "\n",
    "    precision = np.zeros(num_tags)\n",
    "\n",
    "    cm = confusion_matrix(true, pred, num_tags)\n",
    "\n",
    "    for i in range(num_tags):\n",
    "        tp = cm[i][i]\n",
    "        tp_fp = np.sum(cm[:, i])\n",
    "        precision[i] = tp / tp_fp if tp_fp > 0 else 0\n",
    "\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJL55TnOBVxl"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"def recall(true, pred, num_tags):\\n    \\\"\\\"\\\"\\n    Arguments:\\n    - true:       a list of true label values (integers)\\n    - pred:       a list of predicted label values (integers)\\n    - num_tags:   the number of possible tags\\n                true and pred will both contain integers between\\n                0 and num_tags - 1 (inclusive)\\n\\n    Output: \\n    - recall:     an array of length num_tags, where recall[i]\\n                gives the recall of class i\\n\\n    Hints:  the confusion matrix may be useful\\n          be careful about zero division\\n    \\\"\\\"\\\"\\n\\n    recall = np.zeros(num_tags)\\n\\n    cm = confusion_matrix(true, pred, num_tags)\\n\\n    for i in range(num_tags):\\n        tp = cm[i][i]\\n        tp_fn = np.sum(cm[i, :])\\n        recall[i] = tp / tp_fn if tp_fn > 0 else 0\\n\\n    return recall\";\n",
       "                var nbb_formatted_code = \"def recall(true, pred, num_tags):\\n    \\\"\\\"\\\"\\n    Arguments:\\n    - true:       a list of true label values (integers)\\n    - pred:       a list of predicted label values (integers)\\n    - num_tags:   the number of possible tags\\n                true and pred will both contain integers between\\n                0 and num_tags - 1 (inclusive)\\n\\n    Output: \\n    - recall:     an array of length num_tags, where recall[i]\\n                gives the recall of class i\\n\\n    Hints:  the confusion matrix may be useful\\n          be careful about zero division\\n    \\\"\\\"\\\"\\n\\n    recall = np.zeros(num_tags)\\n\\n    cm = confusion_matrix(true, pred, num_tags)\\n\\n    for i in range(num_tags):\\n        tp = cm[i][i]\\n        tp_fn = np.sum(cm[i, :])\\n        recall[i] = tp / tp_fn if tp_fn > 0 else 0\\n\\n    return recall\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def recall(true, pred, num_tags):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - true:       a list of true label values (integers)\n",
    "    - pred:       a list of predicted label values (integers)\n",
    "    - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "    Output: \n",
    "    - recall:     an array of length num_tags, where recall[i]\n",
    "                gives the recall of class i\n",
    "\n",
    "    Hints:  the confusion matrix may be useful\n",
    "          be careful about zero division\n",
    "    \"\"\"\n",
    "\n",
    "    recall = np.zeros(num_tags)\n",
    "\n",
    "    cm = confusion_matrix(true, pred, num_tags)\n",
    "\n",
    "    for i in range(num_tags):\n",
    "        tp = cm[i][i]\n",
    "        tp_fn = np.sum(cm[i, :])\n",
    "        recall[i] = tp / tp_fn if tp_fn > 0 else 0\n",
    "\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7drr7z1VBVjD"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"def f1_score(true, pred, num_tags):\\n    \\\"\\\"\\\"\\n    Arguments:\\n    - true:       a list of true label values (integers)\\n    - pred:       a list of predicted label values (integers)\\n    - num_tags:   the number of possible tags\\n                true and pred will both contain integers between\\n                0 and num_tags - 1 (inclusive)\\n\\n    Output: \\n    - f1:         an array of length num_tags, where f1[i]\\n                gives the recall of class i\\n    \\\"\\\"\\\"\\n    f1 = np.zeros(num_tags)\\n\\n    p = precision(true, pred, num_tags)\\n    r = recall(true, pred, num_tags)\\n\\n    for i in range(num_tags):\\n        if p[i] + r[i] > 0:\\n            f1[i] = 2 * (p[i] * r[i]) / (p[i] + r[i])\\n\\n    return f1\";\n",
       "                var nbb_formatted_code = \"def f1_score(true, pred, num_tags):\\n    \\\"\\\"\\\"\\n    Arguments:\\n    - true:       a list of true label values (integers)\\n    - pred:       a list of predicted label values (integers)\\n    - num_tags:   the number of possible tags\\n                true and pred will both contain integers between\\n                0 and num_tags - 1 (inclusive)\\n\\n    Output: \\n    - f1:         an array of length num_tags, where f1[i]\\n                gives the recall of class i\\n    \\\"\\\"\\\"\\n    f1 = np.zeros(num_tags)\\n\\n    p = precision(true, pred, num_tags)\\n    r = recall(true, pred, num_tags)\\n\\n    for i in range(num_tags):\\n        if p[i] + r[i] > 0:\\n            f1[i] = 2 * (p[i] * r[i]) / (p[i] + r[i])\\n\\n    return f1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f1_score(true, pred, num_tags):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - true:       a list of true label values (integers)\n",
    "    - pred:       a list of predicted label values (integers)\n",
    "    - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "    Output: \n",
    "    - f1:         an array of length num_tags, where f1[i]\n",
    "                gives the recall of class i\n",
    "    \"\"\"\n",
    "    f1 = np.zeros(num_tags)\n",
    "\n",
    "    p = precision(true, pred, num_tags)\n",
    "    r = recall(true, pred, num_tags)\n",
    "\n",
    "    for i in range(num_tags):\n",
    "        if p[i] + r[i] > 0:\n",
    "            f1[i] = 2 * (p[i] * r[i]) / (p[i] + r[i])\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "c0_cuugK4972",
    "outputId": "dc8f5cf0-9407-465e-ccf1-62dcb5462412",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 4.]\n",
      " [0. 5.]]\n",
      "[0.2 1. ]\n",
      "[1.         0.55555556]\n",
      "[0.33333333 0.71428571]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# test_case\\ntrue = [1, 1, 0, 1, 0, 0, 0, 1, 1, 0]\\npred = [1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\\nnum_tags = 2\\n\\nprint(confusion_matrix(true, pred, num_tags))\\nprint(recall(true, pred, num_tags))\\nprint(precision(true, pred, num_tags))\\nprint(f1_score(true, pred, num_tags))\";\n",
       "                var nbb_formatted_code = \"# test_case\\ntrue = [1, 1, 0, 1, 0, 0, 0, 1, 1, 0]\\npred = [1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\\nnum_tags = 2\\n\\nprint(confusion_matrix(true, pred, num_tags))\\nprint(recall(true, pred, num_tags))\\nprint(precision(true, pred, num_tags))\\nprint(f1_score(true, pred, num_tags))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test_case\n",
    "true = [1, 1, 0, 1, 0, 0, 0, 1, 1, 0]\n",
    "pred = [1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
    "num_tags = 2\n",
    "\n",
    "print(confusion_matrix(true, pred, num_tags))\n",
    "print(recall(true, pred, num_tags))\n",
    "print(precision(true, pred, num_tags))\n",
    "print(f1_score(true, pred, num_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GS1p55P5UGv4"
   },
   "source": [
    "### Part 3: Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6IPoFwqfoFOO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fill in the blanks in `LSTMTagger`'s `__init__` function. If you get stuck, you can reference PyTorch's [torch.nn documentation](https://pytorch.org/docs/stable/nn.html) or [this official tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) on LSTM sequence labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6J3z3T0USI8"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"class LSTMTagger(nn.Module):\\n    \\\"\\\"\\\"\\n    An LSTM model for sequence labeling\\n\\n    Initialization Arguments:\\n    - embeddings:   a matrix of size (vocab_size, emb_dim)\\n                    containing pretrained embedding weights\\n    - hidden_dim:   the LSTM's hidden layer size\\n    - tagset_size:  the number of possible output tags\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, embeddings, hidden_dim, tagset_size):\\n        super().__init__()\\n\\n        self.hidden_dim = hidden_dim\\n        self.num_labels = tagset_size\\n\\n        # Initialize a PyTorch embeddings layer using the pretrained embedding weights\\n        self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False)\\n\\n        # Initialize an LSTM layer\\n        embedding_dim = int(embeddings.size()[1])\\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\\n\\n        # Initialize a single feedforward layer\\n        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\\n\\n    def forward(self, indices, lengths):\\n        \\\"\\\"\\\"\\n        Runs a batched sequence through the model and returns output logits\\n\\n        Arguments:\\n        - indices:  a matrix of size (batch_size x max_seq_len)\\n                    containing the word indices of sentences in the batch\\n        - lengths:  a vector of size (batch_size) containing the\\n                    original lengths of the sequences before padding\\n\\n        Output:\\n        - logits:   a matrix of size (batch_size x max_seq_len x num_tags)\\n                    gives a score to each possible tag for each word\\n                    in each sentence \\n        \\\"\\\"\\\"\\n        device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\\n        # cast arrays as PyTorch data types and move to GPU memory\\n        indices = torch.LongTensor(indices).to(device)\\n        lengths = torch.LongTensor(lengths).to(device)\\n\\n        # convert word indices to word embeddings\\n        embeddings = self.embeddings(indices)\\n\\n        # pack/pad handles variable length sequence batching\\n        # see here if you're curious: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\\n        packed_input_embs = pack_padded_sequence(\\n            embeddings, lengths, batch_first=True, enforce_sorted=False\\n        )\\n        # run input through LSTM layer\\n        packed_output, _ = self.lstm(packed_input_embs)\\n        # unpack sequences into original format\\n        padded_output, output_lengths = pad_packed_sequence(\\n            packed_output, batch_first=True\\n        )\\n\\n        logits = self.hidden2tag(padded_output)\\n        return logits\\n\\n    def run_training(\\n        self,\\n        train_dataset,\\n        dev_dataset,\\n        batch_size,\\n        vocab,\\n        tagset,\\n        lr=5e-4,\\n        num_epochs=100,\\n        eval_every=5,\\n    ):\\n        \\\"\\\"\\\"\\n        Trains the model on the training data with a learning rate of lr\\n        for num_epochs. Evaluates the model on the dev data eval_every epochs.\\n\\n        Arguments:\\n        - train_dataset:  Dataset object containing the training data\\n        - dev_dataset:    Dataset object containing the dev data\\n        - batch_size:     batch size for train/dev data\\n        - vocab:          a dictionary mapping word strings to indices\\n        - tagset:         a dictionary mapping tag strings to indices\\n        - lr:             learning rate\\n        - num_epochs:     number of epochs to train for\\n        - eval_every:     evaluation is run eval_every epochs\\n        \\\"\\\"\\\"\\n        device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\\n        if str(device) == \\\"cpu\\\":\\n            print(\\\"Training only supported in GPU environment\\\")\\n            return\\n\\n        # clear unreferenced data/models from GPU memory\\n        torch.cuda.empty_cache()\\n        # move model to GPU memory\\n        self.to(device)\\n\\n        # set the optimizer (Adam) and loss function (CrossEnt)\\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\\n        loss_function = nn.CrossEntropyLoss(ignore_index=-100)\\n\\n        # batch training and dev data\\n        train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(\\n            BATCH_SIZE, vocab, tagset\\n        )\\n        dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(\\n            BATCH_SIZE, vocab, tagset\\n        )\\n\\n        print(\\\"**** TRAINING *****\\\")\\n        for i in range(num_epochs):\\n            # sets the model in train mode\\n            self.train()\\n\\n            total_loss = 0\\n            for b in range(len(train_batch_idx)):\\n                # compute the logits\\n                logits = model.forward(train_batch_idx[b], train_batch_lens[b])\\n                # move labels to GPU memory\\n                labels = torch.LongTensor(train_batch_tags[b]).to(device)\\n                # compute the loss with respect to true labels\\n                loss = loss_function(logits.view(-1, len(tagset)), labels.view(-1))\\n                total_loss += loss\\n                # propagate gradients backward\\n                loss.backward()\\n                optimizer.step()\\n                # set model gradients to zero before performing next forward pass\\n                self.zero_grad()\\n\\n            print(\\\"Epoch {} | Loss: {}\\\".format(i, total_loss))\\n\\n            if (i + 1) % eval_every == 0:\\n                print(\\\"**** EVALUATION *****\\\")\\n                # sets the model in evaluate mode (no gradients)\\n                self.eval()\\n                # compute dev f1 score\\n                acc, true, pred = self.evaluate(\\n                    dev_batch_idx, dev_batch_lens, dev_batch_tags, tagset\\n                )\\n                print(\\\"Dev Accuracy: {}\\\".format(acc))\\n                print(\\\"**********************\\\")\\n\\n    def evaluate(self, batched_sentences, batched_lengths, batched_labels, tagset):\\n        \\\"\\\"\\\"\\n        Evaluate the model's predictions on the provided dataset. \\n\\n        Arguments:\\n        - batched_sentences:  a list of matrices, each of size (batch_size x max_seq_len),\\n                            containing the word indices of sentences in the batch\\n        - batched_lengths:    a list of vectors, each of size (batch_size), containing the\\n                            original lengths of the sequences before padding\\n        - batched_labels:     a list of matrices, each of size (batch_size x max_seq_len),\\n                            containing the tag indices corresponding to sentences in the batch\\n        - num_tags:           the number of possible output tags\\n\\n        Output:\\n        - accuracy:           the model's prediction accuracy\\n        - all_true_labels:    a flattened list of all true labels\\n        - all_predictions:    a flattened list of all of the model's corresponding predictions\\n\\n        \\\"\\\"\\\"\\n\\n        all_true_labels = []\\n        all_predictions = []\\n\\n        for b in range(len(batched_sentences)):\\n            logits = self.forward(batched_sentences[b], batched_lengths[b])\\n            batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\\n\\n            batch_size, _ = batched_sentences[b].shape\\n\\n            for i in range(batch_size):\\n                tags = batched_labels[b][i]\\n                preds = batch_predictions[i]\\n\\n                seq_len = int(batched_lengths[b][i])\\n                for j in range(seq_len):\\n                    all_predictions.append(int(preds[j]))\\n                    all_true_labels.append(int(tags[j]))\\n\\n        acc = accuracy(all_true_labels, all_predictions)\\n\\n        return acc, all_true_labels, all_predictions\";\n",
       "                var nbb_formatted_code = \"class LSTMTagger(nn.Module):\\n    \\\"\\\"\\\"\\n    An LSTM model for sequence labeling\\n\\n    Initialization Arguments:\\n    - embeddings:   a matrix of size (vocab_size, emb_dim)\\n                    containing pretrained embedding weights\\n    - hidden_dim:   the LSTM's hidden layer size\\n    - tagset_size:  the number of possible output tags\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, embeddings, hidden_dim, tagset_size):\\n        super().__init__()\\n\\n        self.hidden_dim = hidden_dim\\n        self.num_labels = tagset_size\\n\\n        # Initialize a PyTorch embeddings layer using the pretrained embedding weights\\n        self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False)\\n\\n        # Initialize an LSTM layer\\n        embedding_dim = int(embeddings.size()[1])\\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\\n\\n        # Initialize a single feedforward layer\\n        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\\n\\n    def forward(self, indices, lengths):\\n        \\\"\\\"\\\"\\n        Runs a batched sequence through the model and returns output logits\\n\\n        Arguments:\\n        - indices:  a matrix of size (batch_size x max_seq_len)\\n                    containing the word indices of sentences in the batch\\n        - lengths:  a vector of size (batch_size) containing the\\n                    original lengths of the sequences before padding\\n\\n        Output:\\n        - logits:   a matrix of size (batch_size x max_seq_len x num_tags)\\n                    gives a score to each possible tag for each word\\n                    in each sentence \\n        \\\"\\\"\\\"\\n        device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\\n        # cast arrays as PyTorch data types and move to GPU memory\\n        indices = torch.LongTensor(indices).to(device)\\n        lengths = torch.LongTensor(lengths).to(device)\\n\\n        # convert word indices to word embeddings\\n        embeddings = self.embeddings(indices)\\n\\n        # pack/pad handles variable length sequence batching\\n        # see here if you're curious: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\\n        packed_input_embs = pack_padded_sequence(\\n            embeddings, lengths, batch_first=True, enforce_sorted=False\\n        )\\n        # run input through LSTM layer\\n        packed_output, _ = self.lstm(packed_input_embs)\\n        # unpack sequences into original format\\n        padded_output, output_lengths = pad_packed_sequence(\\n            packed_output, batch_first=True\\n        )\\n\\n        logits = self.hidden2tag(padded_output)\\n        return logits\\n\\n    def run_training(\\n        self,\\n        train_dataset,\\n        dev_dataset,\\n        batch_size,\\n        vocab,\\n        tagset,\\n        lr=5e-4,\\n        num_epochs=100,\\n        eval_every=5,\\n    ):\\n        \\\"\\\"\\\"\\n        Trains the model on the training data with a learning rate of lr\\n        for num_epochs. Evaluates the model on the dev data eval_every epochs.\\n\\n        Arguments:\\n        - train_dataset:  Dataset object containing the training data\\n        - dev_dataset:    Dataset object containing the dev data\\n        - batch_size:     batch size for train/dev data\\n        - vocab:          a dictionary mapping word strings to indices\\n        - tagset:         a dictionary mapping tag strings to indices\\n        - lr:             learning rate\\n        - num_epochs:     number of epochs to train for\\n        - eval_every:     evaluation is run eval_every epochs\\n        \\\"\\\"\\\"\\n        device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\\n        if str(device) == \\\"cpu\\\":\\n            print(\\\"Training only supported in GPU environment\\\")\\n            return\\n\\n        # clear unreferenced data/models from GPU memory\\n        torch.cuda.empty_cache()\\n        # move model to GPU memory\\n        self.to(device)\\n\\n        # set the optimizer (Adam) and loss function (CrossEnt)\\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\\n        loss_function = nn.CrossEntropyLoss(ignore_index=-100)\\n\\n        # batch training and dev data\\n        train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(\\n            BATCH_SIZE, vocab, tagset\\n        )\\n        dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(\\n            BATCH_SIZE, vocab, tagset\\n        )\\n\\n        print(\\\"**** TRAINING *****\\\")\\n        for i in range(num_epochs):\\n            # sets the model in train mode\\n            self.train()\\n\\n            total_loss = 0\\n            for b in range(len(train_batch_idx)):\\n                # compute the logits\\n                logits = model.forward(train_batch_idx[b], train_batch_lens[b])\\n                # move labels to GPU memory\\n                labels = torch.LongTensor(train_batch_tags[b]).to(device)\\n                # compute the loss with respect to true labels\\n                loss = loss_function(logits.view(-1, len(tagset)), labels.view(-1))\\n                total_loss += loss\\n                # propagate gradients backward\\n                loss.backward()\\n                optimizer.step()\\n                # set model gradients to zero before performing next forward pass\\n                self.zero_grad()\\n\\n            print(\\\"Epoch {} | Loss: {}\\\".format(i, total_loss))\\n\\n            if (i + 1) % eval_every == 0:\\n                print(\\\"**** EVALUATION *****\\\")\\n                # sets the model in evaluate mode (no gradients)\\n                self.eval()\\n                # compute dev f1 score\\n                acc, true, pred = self.evaluate(\\n                    dev_batch_idx, dev_batch_lens, dev_batch_tags, tagset\\n                )\\n                print(\\\"Dev Accuracy: {}\\\".format(acc))\\n                print(\\\"**********************\\\")\\n\\n    def evaluate(self, batched_sentences, batched_lengths, batched_labels, tagset):\\n        \\\"\\\"\\\"\\n        Evaluate the model's predictions on the provided dataset. \\n\\n        Arguments:\\n        - batched_sentences:  a list of matrices, each of size (batch_size x max_seq_len),\\n                            containing the word indices of sentences in the batch\\n        - batched_lengths:    a list of vectors, each of size (batch_size), containing the\\n                            original lengths of the sequences before padding\\n        - batched_labels:     a list of matrices, each of size (batch_size x max_seq_len),\\n                            containing the tag indices corresponding to sentences in the batch\\n        - num_tags:           the number of possible output tags\\n\\n        Output:\\n        - accuracy:           the model's prediction accuracy\\n        - all_true_labels:    a flattened list of all true labels\\n        - all_predictions:    a flattened list of all of the model's corresponding predictions\\n\\n        \\\"\\\"\\\"\\n\\n        all_true_labels = []\\n        all_predictions = []\\n\\n        for b in range(len(batched_sentences)):\\n            logits = self.forward(batched_sentences[b], batched_lengths[b])\\n            batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\\n\\n            batch_size, _ = batched_sentences[b].shape\\n\\n            for i in range(batch_size):\\n                tags = batched_labels[b][i]\\n                preds = batch_predictions[i]\\n\\n                seq_len = int(batched_lengths[b][i])\\n                for j in range(seq_len):\\n                    all_predictions.append(int(preds[j]))\\n                    all_true_labels.append(int(tags[j]))\\n\\n        acc = accuracy(all_true_labels, all_predictions)\\n\\n        return acc, all_true_labels, all_predictions\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    An LSTM model for sequence labeling\n",
    "\n",
    "    Initialization Arguments:\n",
    "    - embeddings:   a matrix of size (vocab_size, emb_dim)\n",
    "                    containing pretrained embedding weights\n",
    "    - hidden_dim:   the LSTM's hidden layer size\n",
    "    - tagset_size:  the number of possible output tags\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embeddings, hidden_dim, tagset_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_labels = tagset_size\n",
    "\n",
    "        # Initialize a PyTorch embeddings layer using the pretrained embedding weights\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False)\n",
    "\n",
    "        # Initialize an LSTM layer\n",
    "        embedding_dim = int(embeddings.size()[1])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Initialize a single feedforward layer\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, indices, lengths):\n",
    "        \"\"\"\n",
    "        Runs a batched sequence through the model and returns output logits\n",
    "\n",
    "        Arguments:\n",
    "        - indices:  a matrix of size (batch_size x max_seq_len)\n",
    "                    containing the word indices of sentences in the batch\n",
    "        - lengths:  a vector of size (batch_size) containing the\n",
    "                    original lengths of the sequences before padding\n",
    "\n",
    "        Output:\n",
    "        - logits:   a matrix of size (batch_size x max_seq_len x num_tags)\n",
    "                    gives a score to each possible tag for each word\n",
    "                    in each sentence \n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # cast arrays as PyTorch data types and move to GPU memory\n",
    "        indices = torch.LongTensor(indices).to(device)\n",
    "        lengths = torch.LongTensor(lengths).to(device)\n",
    "\n",
    "        # convert word indices to word embeddings\n",
    "        embeddings = self.embeddings(indices)\n",
    "\n",
    "        # pack/pad handles variable length sequence batching\n",
    "        # see here if you're curious: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "        packed_input_embs = pack_padded_sequence(\n",
    "            embeddings, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        # run input through LSTM layer\n",
    "        packed_output, _ = self.lstm(packed_input_embs)\n",
    "        # unpack sequences into original format\n",
    "        padded_output, output_lengths = pad_packed_sequence(\n",
    "            packed_output, batch_first=True\n",
    "        )\n",
    "\n",
    "        logits = self.hidden2tag(padded_output)\n",
    "        return logits\n",
    "\n",
    "    def run_training(\n",
    "        self,\n",
    "        train_dataset,\n",
    "        dev_dataset,\n",
    "        batch_size,\n",
    "        vocab,\n",
    "        tagset,\n",
    "        lr=5e-4,\n",
    "        num_epochs=100,\n",
    "        eval_every=5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the model on the training data with a learning rate of lr\n",
    "        for num_epochs. Evaluates the model on the dev data eval_every epochs.\n",
    "\n",
    "        Arguments:\n",
    "        - train_dataset:  Dataset object containing the training data\n",
    "        - dev_dataset:    Dataset object containing the dev data\n",
    "        - batch_size:     batch size for train/dev data\n",
    "        - vocab:          a dictionary mapping word strings to indices\n",
    "        - tagset:         a dictionary mapping tag strings to indices\n",
    "        - lr:             learning rate\n",
    "        - num_epochs:     number of epochs to train for\n",
    "        - eval_every:     evaluation is run eval_every epochs\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if str(device) == \"cpu\":\n",
    "            print(\"Training only supported in GPU environment\")\n",
    "            return\n",
    "\n",
    "        # clear unreferenced data/models from GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        # move model to GPU memory\n",
    "        self.to(device)\n",
    "\n",
    "        # set the optimizer (Adam) and loss function (CrossEnt)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_function = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "        # batch training and dev data\n",
    "        train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(\n",
    "            BATCH_SIZE, vocab, tagset\n",
    "        )\n",
    "        dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(\n",
    "            BATCH_SIZE, vocab, tagset\n",
    "        )\n",
    "\n",
    "        print(\"**** TRAINING *****\")\n",
    "        for i in range(num_epochs):\n",
    "            # sets the model in train mode\n",
    "            self.train()\n",
    "\n",
    "            total_loss = 0\n",
    "            for b in range(len(train_batch_idx)):\n",
    "                # compute the logits\n",
    "                logits = model.forward(train_batch_idx[b], train_batch_lens[b])\n",
    "                # move labels to GPU memory\n",
    "                labels = torch.LongTensor(train_batch_tags[b]).to(device)\n",
    "                # compute the loss with respect to true labels\n",
    "                loss = loss_function(logits.view(-1, len(tagset)), labels.view(-1))\n",
    "                total_loss += loss\n",
    "                # propagate gradients backward\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # set model gradients to zero before performing next forward pass\n",
    "                self.zero_grad()\n",
    "\n",
    "            print(\"Epoch {} | Loss: {}\".format(i, total_loss))\n",
    "\n",
    "            if (i + 1) % eval_every == 0:\n",
    "                print(\"**** EVALUATION *****\")\n",
    "                # sets the model in evaluate mode (no gradients)\n",
    "                self.eval()\n",
    "                # compute dev f1 score\n",
    "                acc, true, pred = self.evaluate(\n",
    "                    dev_batch_idx, dev_batch_lens, dev_batch_tags, tagset\n",
    "                )\n",
    "                print(\"Dev Accuracy: {}\".format(acc))\n",
    "                print(\"**********************\")\n",
    "\n",
    "    def evaluate(self, batched_sentences, batched_lengths, batched_labels, tagset):\n",
    "        \"\"\"\n",
    "        Evaluate the model's predictions on the provided dataset. \n",
    "\n",
    "        Arguments:\n",
    "        - batched_sentences:  a list of matrices, each of size (batch_size x max_seq_len),\n",
    "                            containing the word indices of sentences in the batch\n",
    "        - batched_lengths:    a list of vectors, each of size (batch_size), containing the\n",
    "                            original lengths of the sequences before padding\n",
    "        - batched_labels:     a list of matrices, each of size (batch_size x max_seq_len),\n",
    "                            containing the tag indices corresponding to sentences in the batch\n",
    "        - num_tags:           the number of possible output tags\n",
    "\n",
    "        Output:\n",
    "        - accuracy:           the model's prediction accuracy\n",
    "        - all_true_labels:    a flattened list of all true labels\n",
    "        - all_predictions:    a flattened list of all of the model's corresponding predictions\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        all_true_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        for b in range(len(batched_sentences)):\n",
    "            logits = self.forward(batched_sentences[b], batched_lengths[b])\n",
    "            batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            batch_size, _ = batched_sentences[b].shape\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                tags = batched_labels[b][i]\n",
    "                preds = batch_predictions[i]\n",
    "\n",
    "                seq_len = int(batched_lengths[b][i])\n",
    "                for j in range(seq_len):\n",
    "                    all_predictions.append(int(preds[j]))\n",
    "                    all_true_labels.append(int(tags[j]))\n",
    "\n",
    "        acc = accuracy(all_true_labels, all_predictions)\n",
    "\n",
    "        return acc, all_true_labels, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4AOB94R9RFo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"def set_seed(seed):\\n    \\\"\\\"\\\"\\n    Sets random seeds and sets model in deterministic\\n    training mode. Ensures reproducible results\\n    \\\"\\\"\\\"\\n    torch.manual_seed(seed)\\n    torch.backends.cudnn.deterministic = True\\n    torch.backends.cudnn.benchmark = False\\n    np.random.seed(seed)\";\n",
       "                var nbb_formatted_code = \"def set_seed(seed):\\n    \\\"\\\"\\\"\\n    Sets random seeds and sets model in deterministic\\n    training mode. Ensures reproducible results\\n    \\\"\\\"\\\"\\n    torch.manual_seed(seed)\\n    torch.backends.cudnn.deterministic = True\\n    torch.backends.cudnn.benchmark = False\\n    np.random.seed(seed)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets random seeds and sets model in deterministic\n",
    "    training mode. Ensures reproducible results\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-AuNeDk9qAM_"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WTEvLeVuNWl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run the cells below to train your model. If all of the previous sections are implemented correctly, you should see\n",
    "\n",
    "\n",
    "*   the loss decreasing consistently for every epoch\n",
    "*   the dev accuracy increasing until convergence around ~0.88\n",
    "\n",
    "The staff solution achieves an accuracy of 0.880 after 25 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "S9NqwYnfU2WB",
    "outputId": "f655d3f2-e46e-420f-b6db-a7f3cf342e9f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** TRAINING *****\n",
      "Epoch 0 | Loss: 999.9422607421875\n",
      "Epoch 1 | Loss: 442.25811767578125\n",
      "Epoch 2 | Loss: 275.37127685546875\n",
      "Epoch 3 | Loss: 212.16624450683594\n",
      "Epoch 4 | Loss: 181.4229736328125\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.8586200039769338\n",
      "**********************\n",
      "Epoch 5 | Loss: 163.11807250976562\n",
      "Epoch 6 | Loss: 150.56387329101562\n",
      "Epoch 7 | Loss: 141.14605712890625\n",
      "Epoch 8 | Loss: 133.68817138671875\n",
      "Epoch 9 | Loss: 127.54783630371094\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.8754026645456353\n",
      "**********************\n",
      "Epoch 10 | Loss: 122.3573989868164\n",
      "Epoch 11 | Loss: 117.87614440917969\n",
      "Epoch 12 | Loss: 113.92591094970703\n",
      "Epoch 13 | Loss: 110.38809967041016\n",
      "Epoch 14 | Loss: 107.1690444946289\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.8791409823026447\n",
      "**********************\n",
      "Epoch 15 | Loss: 104.20204162597656\n",
      "Epoch 16 | Loss: 101.43944549560547\n",
      "Epoch 17 | Loss: 98.83808135986328\n",
      "Epoch 18 | Loss: 96.3730239868164\n",
      "Epoch 19 | Loss: 94.02762603759766\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.8803738317757009\n",
      "**********************\n",
      "Epoch 20 | Loss: 91.78711700439453\n",
      "Epoch 21 | Loss: 89.63475036621094\n",
      "Epoch 22 | Loss: 87.56657409667969\n",
      "Epoch 23 | Loss: 85.56128692626953\n",
      "Epoch 24 | Loss: 83.62176513671875\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.8802147544243388\n",
      "**********************\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 112;\n",
       "                var nbb_unformatted_code = \"# sets the random seed \\u2013\\u00a0DO NOT change this\\n# this ensures deterministic results that are comparable with the staff values\\nset_seed(159)\\n\\nHIDDEN_SIZE = 64\\n# intialize a new LSTMTagger model\\nmodel = LSTMTagger(embeddings, HIDDEN_SIZE, len(tagset))\\n# train the model\\nmodel.run_training(\\n    train_dataset,\\n    dev_dataset,\\n    BATCH_SIZE,\\n    vocab,\\n    tagset,\\n    lr=5e-4,\\n    num_epochs=25,\\n    eval_every=5,\\n)\";\n",
       "                var nbb_formatted_code = \"# sets the random seed \\u2013\\u00a0DO NOT change this\\n# this ensures deterministic results that are comparable with the staff values\\nset_seed(159)\\n\\nHIDDEN_SIZE = 64\\n# intialize a new LSTMTagger model\\nmodel = LSTMTagger(embeddings, HIDDEN_SIZE, len(tagset))\\n# train the model\\nmodel.run_training(\\n    train_dataset,\\n    dev_dataset,\\n    BATCH_SIZE,\\n    vocab,\\n    tagset,\\n    lr=5e-4,\\n    num_epochs=25,\\n    eval_every=5,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sets the random seed – DO NOT change this\n",
    "# this ensures deterministic results that are comparable with the staff values\n",
    "set_seed(159)\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "# intialize a new LSTMTagger model\n",
    "model = LSTMTagger(embeddings, HIDDEN_SIZE, len(tagset))\n",
    "# train the model\n",
    "model.run_training(\n",
    "    train_dataset,\n",
    "    dev_dataset,\n",
    "    BATCH_SIZE,\n",
    "    vocab,\n",
    "    tagset,\n",
    "    lr=5e-4,\n",
    "    num_epochs=25,\n",
    "    eval_every=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JH5giKgvJp0c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once the model is trained, run the cells below to print the precision, recall, and $F_1$ score per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLqaZ_6cMMPM"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 109;\n",
       "                var nbb_unformatted_code = \"def eval_per_class(model, dataset, vocab, tagset):\\n    \\\"\\\"\\\"\\n    Prints precision, recall, and F1 for each class in the tagset\\n    \\\"\\\"\\\"\\n    # batch the data\\n    batched_idx, batched_tags, batched_lens = dev_dataset.get_batches(\\n        BATCH_SIZE, vocab, tagset\\n    )\\n    # compute idx --> tag from tag --> idx\\n    reverse_tagset = {v: k for k, v in tagset.items()}\\n    # evaluate model on hold-out set\\n    acc, true, pred = model.evaluate(batched_idx, batched_lens, batched_tags, tagset)\\n    true = np.array(true)\\n    pred = np.array(pred)\\n\\n    pr = precision(true, pred, len(tagset))\\n    re = recall(true, pred, len(tagset))\\n    f1 = f1_score(true, pred, len(tagset))\\n\\n    for idx, tag in reverse_tagset.items():\\n        print(\\\"***********************\\\")\\n        print(\\\"TAG: {}\\\".format(tag))\\n        num_pred = np.sum(pred == idx)\\n        num_true = np.sum(true == idx)\\n        print(\\\"({} pred, {} true)\\\".format(num_pred, num_true))\\n\\n        print(\\\"PRECISION: \\\\t{:.3f}\\\".format(pr[idx]))\\n        print(\\\"RECALL: \\\\t{:.3f}\\\".format(re[idx]))\\n        print(\\\"F1 SCORE: \\\\t{:.3f}\\\".format(f1[idx]))\";\n",
       "                var nbb_formatted_code = \"def eval_per_class(model, dataset, vocab, tagset):\\n    \\\"\\\"\\\"\\n    Prints precision, recall, and F1 for each class in the tagset\\n    \\\"\\\"\\\"\\n    # batch the data\\n    batched_idx, batched_tags, batched_lens = dev_dataset.get_batches(\\n        BATCH_SIZE, vocab, tagset\\n    )\\n    # compute idx --> tag from tag --> idx\\n    reverse_tagset = {v: k for k, v in tagset.items()}\\n    # evaluate model on hold-out set\\n    acc, true, pred = model.evaluate(batched_idx, batched_lens, batched_tags, tagset)\\n    true = np.array(true)\\n    pred = np.array(pred)\\n\\n    pr = precision(true, pred, len(tagset))\\n    re = recall(true, pred, len(tagset))\\n    f1 = f1_score(true, pred, len(tagset))\\n\\n    for idx, tag in reverse_tagset.items():\\n        print(\\\"***********************\\\")\\n        print(\\\"TAG: {}\\\".format(tag))\\n        num_pred = np.sum(pred == idx)\\n        num_true = np.sum(true == idx)\\n        print(\\\"({} pred, {} true)\\\".format(num_pred, num_true))\\n\\n        print(\\\"PRECISION: \\\\t{:.3f}\\\".format(pr[idx]))\\n        print(\\\"RECALL: \\\\t{:.3f}\\\".format(re[idx]))\\n        print(\\\"F1 SCORE: \\\\t{:.3f}\\\".format(f1[idx]))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def eval_per_class(model, dataset, vocab, tagset):\n",
    "    \"\"\"\n",
    "    Prints precision, recall, and F1 for each class in the tagset\n",
    "    \"\"\"\n",
    "    # batch the data\n",
    "    batched_idx, batched_tags, batched_lens = dev_dataset.get_batches(\n",
    "        BATCH_SIZE, vocab, tagset\n",
    "    )\n",
    "    # compute idx --> tag from tag --> idx\n",
    "    reverse_tagset = {v: k for k, v in tagset.items()}\n",
    "    # evaluate model on hold-out set\n",
    "    acc, true, pred = model.evaluate(batched_idx, batched_lens, batched_tags, tagset)\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    pr = precision(true, pred, len(tagset))\n",
    "    re = recall(true, pred, len(tagset))\n",
    "    f1 = f1_score(true, pred, len(tagset))\n",
    "\n",
    "    for idx, tag in reverse_tagset.items():\n",
    "        print(\"***********************\")\n",
    "        print(\"TAG: {}\".format(tag))\n",
    "        num_pred = np.sum(pred == idx)\n",
    "        num_true = np.sum(true == idx)\n",
    "        print(\"({} pred, {} true)\".format(num_pred, num_true))\n",
    "\n",
    "        print(\"PRECISION: \\t{:.3f}\".format(pr[idx]))\n",
    "        print(\"RECALL: \\t{:.3f}\".format(re[idx]))\n",
    "        print(\"F1 SCORE: \\t{:.3f}\".format(f1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-tTEpCBsuYuP",
    "outputId": "980e3693-66f5-47d7-92f0-fbda905ff25e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************\n",
      "TAG: $\n",
      "(14 pred, 14 true)\n",
      "PRECISION: \t1.000\n",
      "RECALL: \t1.000\n",
      "F1 SCORE: \t1.000\n",
      "***********************\n",
      "TAG: ''\n",
      "(91 pred, 88 true)\n",
      "PRECISION: \t0.956\n",
      "RECALL: \t0.989\n",
      "F1 SCORE: \t0.972\n",
      "***********************\n",
      "TAG: ,\n",
      "(937 pred, 936 true)\n",
      "PRECISION: \t0.965\n",
      "RECALL: \t0.966\n",
      "F1 SCORE: \t0.965\n",
      "***********************\n",
      "TAG: -LRB-\n",
      "(122 pred, 117 true)\n",
      "PRECISION: \t0.959\n",
      "RECALL: \t1.000\n",
      "F1 SCORE: \t0.979\n",
      "***********************\n",
      "TAG: -RRB-\n",
      "(125 pred, 120 true)\n",
      "PRECISION: \t0.952\n",
      "RECALL: \t0.992\n",
      "F1 SCORE: \t0.971\n",
      "***********************\n",
      "TAG: .\n",
      "(1512 pred, 1503 true)\n",
      "PRECISION: \t0.987\n",
      "RECALL: \t0.993\n",
      "F1 SCORE: \t0.990\n",
      "***********************\n",
      "TAG: :\n",
      "(103 pred, 106 true)\n",
      "PRECISION: \t0.942\n",
      "RECALL: \t0.915\n",
      "F1 SCORE: \t0.928\n",
      "***********************\n",
      "TAG: ADD\n",
      "(99 pred, 81 true)\n",
      "PRECISION: \t0.576\n",
      "RECALL: \t0.704\n",
      "F1 SCORE: \t0.633\n",
      "***********************\n",
      "TAG: AFX\n",
      "(2 pred, 4 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: CC\n",
      "(776 pred, 781 true)\n",
      "PRECISION: \t0.994\n",
      "RECALL: \t0.987\n",
      "F1 SCORE: \t0.990\n",
      "***********************\n",
      "TAG: CD\n",
      "(376 pred, 378 true)\n",
      "PRECISION: \t0.918\n",
      "RECALL: \t0.913\n",
      "F1 SCORE: \t0.915\n",
      "***********************\n",
      "TAG: DT\n",
      "(1950 pred, 1943 true)\n",
      "PRECISION: \t0.986\n",
      "RECALL: \t0.989\n",
      "F1 SCORE: \t0.987\n",
      "***********************\n",
      "TAG: EX\n",
      "(57 pred, 56 true)\n",
      "PRECISION: \t0.930\n",
      "RECALL: \t0.946\n",
      "F1 SCORE: \t0.938\n",
      "***********************\n",
      "TAG: FW\n",
      "(6 pred, 30 true)\n",
      "PRECISION: \t0.833\n",
      "RECALL: \t0.167\n",
      "F1 SCORE: \t0.278\n",
      "***********************\n",
      "TAG: GW\n",
      "(16 pred, 32 true)\n",
      "PRECISION: \t0.500\n",
      "RECALL: \t0.250\n",
      "F1 SCORE: \t0.333\n",
      "***********************\n",
      "TAG: HYPH\n",
      "(99 pred, 95 true)\n",
      "PRECISION: \t0.859\n",
      "RECALL: \t0.895\n",
      "F1 SCORE: \t0.876\n",
      "***********************\n",
      "TAG: IN\n",
      "(2435 pred, 2353 true)\n",
      "PRECISION: \t0.949\n",
      "RECALL: \t0.983\n",
      "F1 SCORE: \t0.966\n",
      "***********************\n",
      "TAG: JJ\n",
      "(1688 pred, 1655 true)\n",
      "PRECISION: \t0.882\n",
      "RECALL: \t0.899\n",
      "F1 SCORE: \t0.890\n",
      "***********************\n",
      "TAG: JJR\n",
      "(51 pred, 47 true)\n",
      "PRECISION: \t0.784\n",
      "RECALL: \t0.851\n",
      "F1 SCORE: \t0.816\n",
      "***********************\n",
      "TAG: JJS\n",
      "(76 pred, 84 true)\n",
      "PRECISION: \t0.934\n",
      "RECALL: \t0.845\n",
      "F1 SCORE: \t0.888\n",
      "***********************\n",
      "TAG: LS\n",
      "(4 pred, 5 true)\n",
      "PRECISION: \t0.750\n",
      "RECALL: \t0.600\n",
      "F1 SCORE: \t0.667\n",
      "***********************\n",
      "TAG: MD\n",
      "(356 pred, 358 true)\n",
      "PRECISION: \t0.994\n",
      "RECALL: \t0.989\n",
      "F1 SCORE: \t0.992\n",
      "***********************\n",
      "TAG: NFP\n",
      "(59 pred, 60 true)\n",
      "PRECISION: \t0.712\n",
      "RECALL: \t0.700\n",
      "F1 SCORE: \t0.706\n",
      "***********************\n",
      "TAG: NN\n",
      "(3452 pred, 3336 true)\n",
      "PRECISION: \t0.873\n",
      "RECALL: \t0.903\n",
      "F1 SCORE: \t0.888\n",
      "***********************\n",
      "TAG: NNP\n",
      "(1669 pred, 1816 true)\n",
      "PRECISION: \t0.846\n",
      "RECALL: \t0.778\n",
      "F1 SCORE: \t0.810\n",
      "***********************\n",
      "TAG: NNPS\n",
      "(37 pred, 63 true)\n",
      "PRECISION: \t0.622\n",
      "RECALL: \t0.365\n",
      "F1 SCORE: \t0.460\n",
      "***********************\n",
      "TAG: NNS\n",
      "(958 pred, 929 true)\n",
      "PRECISION: \t0.891\n",
      "RECALL: \t0.919\n",
      "F1 SCORE: \t0.905\n",
      "***********************\n",
      "TAG: PDT\n",
      "(22 pred, 21 true)\n",
      "PRECISION: \t0.909\n",
      "RECALL: \t0.952\n",
      "F1 SCORE: \t0.930\n",
      "***********************\n",
      "TAG: POS\n",
      "(87 pred, 84 true)\n",
      "PRECISION: \t0.943\n",
      "RECALL: \t0.976\n",
      "F1 SCORE: \t0.959\n",
      "***********************\n",
      "TAG: PRP\n",
      "(1493 pred, 1487 true)\n",
      "PRECISION: \t0.992\n",
      "RECALL: \t0.996\n",
      "F1 SCORE: \t0.994\n",
      "***********************\n",
      "TAG: PRP$\n",
      "(313 pred, 315 true)\n",
      "PRECISION: \t0.997\n",
      "RECALL: \t0.990\n",
      "F1 SCORE: \t0.994\n",
      "***********************\n",
      "TAG: RB\n",
      "(1277 pred, 1292 true)\n",
      "PRECISION: \t0.907\n",
      "RECALL: \t0.896\n",
      "F1 SCORE: \t0.902\n",
      "***********************\n",
      "TAG: RBR\n",
      "(20 pred, 22 true)\n",
      "PRECISION: \t0.700\n",
      "RECALL: \t0.636\n",
      "F1 SCORE: \t0.667\n",
      "***********************\n",
      "TAG: RBS\n",
      "(20 pred, 20 true)\n",
      "PRECISION: \t0.800\n",
      "RECALL: \t0.800\n",
      "F1 SCORE: \t0.800\n",
      "***********************\n",
      "TAG: RP\n",
      "(56 pred, 75 true)\n",
      "PRECISION: \t0.786\n",
      "RECALL: \t0.587\n",
      "F1 SCORE: \t0.672\n",
      "***********************\n",
      "TAG: SYM\n",
      "(8 pred, 20 true)\n",
      "PRECISION: \t0.875\n",
      "RECALL: \t0.350\n",
      "F1 SCORE: \t0.500\n",
      "***********************\n",
      "TAG: TO\n",
      "(343 pred, 359 true)\n",
      "PRECISION: \t0.980\n",
      "RECALL: \t0.936\n",
      "F1 SCORE: \t0.957\n",
      "***********************\n",
      "TAG: UH\n",
      "(92 pred, 116 true)\n",
      "PRECISION: \t0.859\n",
      "RECALL: \t0.681\n",
      "F1 SCORE: \t0.760\n",
      "***********************\n",
      "TAG: VB\n",
      "(1099 pred, 1122 true)\n",
      "PRECISION: \t0.935\n",
      "RECALL: \t0.916\n",
      "F1 SCORE: \t0.926\n",
      "***********************\n",
      "TAG: VBD\n",
      "(504 pred, 520 true)\n",
      "PRECISION: \t0.938\n",
      "RECALL: \t0.910\n",
      "F1 SCORE: \t0.924\n",
      "***********************\n",
      "TAG: VBG\n",
      "(393 pred, 384 true)\n",
      "PRECISION: \t0.863\n",
      "RECALL: \t0.883\n",
      "F1 SCORE: \t0.873\n",
      "***********************\n",
      "TAG: VBN\n",
      "(516 pred, 476 true)\n",
      "PRECISION: \t0.839\n",
      "RECALL: \t0.910\n",
      "F1 SCORE: \t0.873\n",
      "***********************\n",
      "TAG: VBP\n",
      "(765 pred, 771 true)\n",
      "PRECISION: \t0.942\n",
      "RECALL: \t0.935\n",
      "F1 SCORE: \t0.939\n",
      "***********************\n",
      "TAG: VBZ\n",
      "(648 pred, 643 true)\n",
      "PRECISION: \t0.968\n",
      "RECALL: \t0.975\n",
      "F1 SCORE: \t0.971\n",
      "***********************\n",
      "TAG: WDT\n",
      "(89 pred, 106 true)\n",
      "PRECISION: \t0.933\n",
      "RECALL: \t0.783\n",
      "F1 SCORE: \t0.851\n",
      "***********************\n",
      "TAG: WP\n",
      "(122 pred, 113 true)\n",
      "PRECISION: \t0.918\n",
      "RECALL: \t0.991\n",
      "F1 SCORE: \t0.953\n",
      "***********************\n",
      "TAG: WP$\n",
      "(1 pred, 2 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: WRB\n",
      "(115 pred, 113 true)\n",
      "PRECISION: \t0.974\n",
      "RECALL: \t0.991\n",
      "F1 SCORE: \t0.982\n",
      "***********************\n",
      "TAG: XX\n",
      "(0 pred, 3 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: ``\n",
      "(92 pred, 91 true)\n",
      "PRECISION: \t0.967\n",
      "RECALL: \t0.978\n",
      "F1 SCORE: \t0.973\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 110;\n",
       "                var nbb_unformatted_code = \"eval_per_class(model, dev_dataset, vocab, tagset)\";\n",
       "                var nbb_formatted_code = \"eval_per_class(model, dev_dataset, vocab, tagset)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_per_class(model, dev_dataset, vocab, tagset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lefzFwCD8AJU"
   },
   "source": [
    "## Part 4: Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxZ_Wn75uw7n"
   },
   "source": [
    "Congratulations, you've just trained a neural network!\n",
    "\n",
    "Now, improve the `LSTMTagger` model and implementing the `init` function in the `FancyTagger` class below. \n",
    "* Feel free to replace the `forward` function inherited from `LSTMTagger` if \n",
    "you need to, but it should not be necessary to receive full credit. Credit will be awarded based on the performance on a holdout test set. \n",
    "* Do not modify any of the cells above when completing part 4. Instead, insert cells below if you need to perform any additional computations. \n",
    "* You are allowed to use any function in `torch.nn`. You are **not** allowed to import any libraries or use implementations copied from the internet. \n",
    "\n",
    "Before submitting, please describe your modifications below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Ya-aaGh6l8D",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Reduced number of unknown words by increasing vocab size to max vocab size(400k).\n",
    "2. Increased GloVe embedding size to 300d.\n",
    "3. Used 4 layers of stacked BiLSTM and increased hidden size to 512.\n",
    "4. Used dropout for regularization.\n",
    "5. Increased batch size to 256 for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mh-mX6CHI_AV"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 100;\n",
       "                var nbb_unformatted_code = \"# parameters\\nNUM_LAYERS = 4\\nBI = True\\nDROPOUT = 0.3\\nHIDDEN_SIZE = 512\\n\\nVOCAB_SIZE = 400000\\nEMBEDDING_SIZE = 300\\n\\nBATCH_SIZE = 256\";\n",
       "                var nbb_formatted_code = \"# parameters\\nNUM_LAYERS = 4\\nBI = True\\nDROPOUT = 0.3\\nHIDDEN_SIZE = 512\\n\\nVOCAB_SIZE = 400000\\nEMBEDDING_SIZE = 300\\n\\nBATCH_SIZE = 256\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters\n",
    "NUM_LAYERS = 4\n",
    "BI = True\n",
    "DROPOUT = 0.3\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "VOCAB_SIZE = 400000\n",
    "EMBEDDING_SIZE = 300\n",
    "\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1uUgBHRKC-i"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 101;\n",
       "                var nbb_unformatted_code = \"embeddings, vocab = read_embeddings(f\\\"glove.6B.{EMBEDDING_SIZE}d.txt\\\", VOCAB_SIZE)\";\n",
       "                var nbb_formatted_code = \"embeddings, vocab = read_embeddings(f\\\"glove.6B.{EMBEDDING_SIZE}d.txt\\\", VOCAB_SIZE)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings, vocab = read_embeddings(f\"glove.6B.{EMBEDDING_SIZE}d.txt\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 102;\n",
       "                var nbb_unformatted_code = \"# read the files\\ntagset = read_tagset(\\\"pos.tagset\\\")\\ntrain_dataset = Dataset(\\\"pos.train\\\", is_labeled=True)\\ndev_dataset = Dataset(\\\"pos.dev\\\", is_labeled=True)\\ntest_dataset = Dataset(\\\"pos.test\\\", is_labeled=False)\\n\\n# these should run without errors if implemented correctly\\ntrain_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(\\n    BATCH_SIZE, vocab, tagset\\n)\\ndev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(\\n    BATCH_SIZE, vocab, tagset\\n)\\ntest_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)\";\n",
       "                var nbb_formatted_code = \"# read the files\\ntagset = read_tagset(\\\"pos.tagset\\\")\\ntrain_dataset = Dataset(\\\"pos.train\\\", is_labeled=True)\\ndev_dataset = Dataset(\\\"pos.dev\\\", is_labeled=True)\\ntest_dataset = Dataset(\\\"pos.test\\\", is_labeled=False)\\n\\n# these should run without errors if implemented correctly\\ntrain_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(\\n    BATCH_SIZE, vocab, tagset\\n)\\ndev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(\\n    BATCH_SIZE, vocab, tagset\\n)\\ntest_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the files\n",
    "tagset = read_tagset(\"pos.tagset\")\n",
    "train_dataset = Dataset(\"pos.train\", is_labeled=True)\n",
    "dev_dataset = Dataset(\"pos.dev\", is_labeled=True)\n",
    "test_dataset = Dataset(\"pos.test\", is_labeled=False)\n",
    "\n",
    "# these should run without errors if implemented correctly\n",
    "train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(\n",
    "    BATCH_SIZE, vocab, tagset\n",
    ")\n",
    "dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(\n",
    "    BATCH_SIZE, vocab, tagset\n",
    ")\n",
    "test_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unknown words: 2440\n",
      "number of unknown words: 463\n",
      "number of unknown words: 522\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 103;\n",
       "                var nbb_unformatted_code = \"# check number of unknown words\\nfor dataset in [train_batch_idx, dev_batch_idx, test_batch_idx]:\\n    count = 0\\n    for x in dataset:\\n        count += list(x.flatten()).count(1)\\n    print(f\\\"number of unknown words: {count}\\\")\";\n",
       "                var nbb_formatted_code = \"# check number of unknown words\\nfor dataset in [train_batch_idx, dev_batch_idx, test_batch_idx]:\\n    count = 0\\n    for x in dataset:\\n        count += list(x.flatten()).count(1)\\n    print(f\\\"number of unknown words: {count}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check number of unknown words\n",
    "for dataset in [train_batch_idx, dev_batch_idx, test_batch_idx]:\n",
    "    count = 0\n",
    "    for x in dataset:\n",
    "        count += list(x.flatten()).count(1)\n",
    "    print(f\"number of unknown words: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKz2PLbu5d8Y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 104;\n",
       "                var nbb_unformatted_code = \"class FancyTagger(LSTMTagger):\\n    \\\"\\\"\\\"\\n  An improved neural model for sequence labeling\\n\\n  Starter code from LSTMTagger has already been provided, but\\n  feel free to change the init and forward function internals\\n  if your model design requires it (though this is not necessary\\n  to receive full credit).\\n\\n  You may use any component in torch.nn. You may NOT\\n  import any additional libraries/modules. \\n\\n  \\\"\\\"\\\"\\n\\n    def __init__(self, embeddings, embedding_dim, hidden_dim, tagset_size):\\n        # initializes the parent LSTMTagger class\\n        # inherits forward, evaluate, and run_training methods\\n        super().__init__(embeddings, hidden_dim, tagset_size)\\n\\n        self.hidden_dim = hidden_dim\\n        self.num_labels = tagset_size\\n\\n        # Initialize a PyTorch embeddings layer using the pretrained embedding weights\\n        self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False)\\n\\n        # Initialize an Bi-LSTM layer\\n        self.lstm = nn.LSTM(\\n            embedding_dim,\\n            hidden_dim,\\n            batch_first=True,\\n            num_layers=NUM_LAYERS,\\n            dropout=DROPOUT,\\n            bidirectional=BI,\\n        )\\n\\n        # Initialize a feedforward layer\\n        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\";\n",
       "                var nbb_formatted_code = \"class FancyTagger(LSTMTagger):\\n    \\\"\\\"\\\"\\n  An improved neural model for sequence labeling\\n\\n  Starter code from LSTMTagger has already been provided, but\\n  feel free to change the init and forward function internals\\n  if your model design requires it (though this is not necessary\\n  to receive full credit).\\n\\n  You may use any component in torch.nn. You may NOT\\n  import any additional libraries/modules. \\n\\n  \\\"\\\"\\\"\\n\\n    def __init__(self, embeddings, embedding_dim, hidden_dim, tagset_size):\\n        # initializes the parent LSTMTagger class\\n        # inherits forward, evaluate, and run_training methods\\n        super().__init__(embeddings, hidden_dim, tagset_size)\\n\\n        self.hidden_dim = hidden_dim\\n        self.num_labels = tagset_size\\n\\n        # Initialize a PyTorch embeddings layer using the pretrained embedding weights\\n        self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False)\\n\\n        # Initialize an Bi-LSTM layer\\n        self.lstm = nn.LSTM(\\n            embedding_dim,\\n            hidden_dim,\\n            batch_first=True,\\n            num_layers=NUM_LAYERS,\\n            dropout=DROPOUT,\\n            bidirectional=BI,\\n        )\\n\\n        # Initialize a feedforward layer\\n        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class FancyTagger(LSTMTagger):\n",
    "    \"\"\"\n",
    "  An improved neural model for sequence labeling\n",
    "\n",
    "  Starter code from LSTMTagger has already been provided, but\n",
    "  feel free to change the init and forward function internals\n",
    "  if your model design requires it (though this is not necessary\n",
    "  to receive full credit).\n",
    "\n",
    "  You may use any component in torch.nn. You may NOT\n",
    "  import any additional libraries/modules. \n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, embeddings, embedding_dim, hidden_dim, tagset_size):\n",
    "        # initializes the parent LSTMTagger class\n",
    "        # inherits forward, evaluate, and run_training methods\n",
    "        super().__init__(embeddings, hidden_dim, tagset_size)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_labels = tagset_size\n",
    "\n",
    "        # Initialize a PyTorch embeddings layer using the pretrained embedding weights\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False)\n",
    "\n",
    "        # Initialize an Bi-LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            dropout=DROPOUT,\n",
    "            bidirectional=BI,\n",
    "        )\n",
    "\n",
    "        # Initialize a feedforward layer\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGDY4ymJvo3h",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run the training script below to train the `FancyTagger` model. Again, feel free to adjust any hyperparameters if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = FancyTagger(embeddings, EMBEDDING_SIZE, HIDDEN_SIZE, len(tagset))\n",
    "print(model)\n",
    "model.run_training(\n",
    "    train_dataset,\n",
    "    dev_dataset,\n",
    "    BATCH_SIZE,\n",
    "    vocab,\n",
    "    tagset,\n",
    "    lr=5e-4,\n",
    "    num_epochs=40,\n",
    "    eval_every=5,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove gpu memory\n",
    "# del embeddings\n",
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save Predictions\n",
    "\n",
    "When you are satisfied with your `FancyTagger`'s performance on the dev set, run the cell below to write your predictions on the test set to a text file. \n",
    "\n",
    "You can download `predictions.txt` by going to \n",
    "**View > Table of Contents > Files**\n",
    "\n",
    "Please submit this `predictions.txt` file to Gradescope. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert isinstance(\n",
    "    model, FancyTagger\n",
    "), \"Please assign your FancyTagger to a variable named model\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "test_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for b in range(len(test_batch_idx)):\n",
    "    logits = model.forward(test_batch_idx[b], test_batch_lens[b])\n",
    "    batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    batch_size, _ = test_batch_idx[b].shape\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        preds = batch_predictions[i]\n",
    "\n",
    "        seq_len = int(test_batch_lens[b][i])\n",
    "        for j in range(seq_len):\n",
    "            predictions.append(int(preds[j]))\n",
    "\n",
    "\n",
    "with open(\"predictions.txt\", \"w\") as f:\n",
    "    for p in predictions:\n",
    "        f.write(str(p) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "8lnp-tWl9Vbo",
    "outputId": "b084b0fc-67f7-498b-f401-9f6a33fe0e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FancyTagger(\n",
      "  (embeddings): Embedding(400000, 300)\n",
      "  (lstm): LSTM(300, 512, num_layers=4, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (hidden2tag): Linear(in_features=1024, out_features=50, bias=True)\n",
      ")\n",
      "**** TRAINING *****\n",
      "Epoch 0 | Loss: 144.7250213623047\n",
      "Epoch 1 | Loss: 81.71239471435547\n",
      "Epoch 2 | Loss: 37.13271713256836\n",
      "Epoch 3 | Loss: 21.516273498535156\n",
      "Epoch 4 | Loss: 15.11073112487793\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9001391926824418\n",
      "**********************\n",
      "Epoch 5 | Loss: 11.63393497467041\n",
      "Epoch 6 | Loss: 9.56954574584961\n",
      "Epoch 7 | Loss: 8.27454948425293\n",
      "Epoch 8 | Loss: 7.372381210327148\n",
      "Epoch 9 | Loss: 6.488227844238281\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9179956253728375\n",
      "**********************\n",
      "Epoch 10 | Loss: 5.761561870574951\n",
      "Epoch 11 | Loss: 5.354988098144531\n",
      "Epoch 12 | Loss: 5.03249979019165\n",
      "Epoch 13 | Loss: 4.656944751739502\n",
      "Epoch 14 | Loss: 4.516438961029053\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.920381785643269\n",
      "**********************\n",
      "Epoch 15 | Loss: 3.823225975036621\n",
      "Epoch 16 | Loss: 3.385925769805908\n",
      "Epoch 17 | Loss: 3.037414073944092\n",
      "Epoch 18 | Loss: 2.703514814376831\n",
      "Epoch 19 | Loss: 2.430333137512207\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9225293298866574\n",
      "**********************\n",
      "Epoch 20 | Loss: 2.1781957149505615\n",
      "Epoch 21 | Loss: 2.0380048751831055\n",
      "Epoch 22 | Loss: 2.0259947776794434\n",
      "Epoch 23 | Loss: 2.1324710845947266\n",
      "Epoch 24 | Loss: 2.0775232315063477\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9222509445217737\n",
      "**********************\n",
      "Epoch 25 | Loss: 1.8133318424224854\n",
      "Epoch 26 | Loss: 1.4007710218429565\n",
      "Epoch 27 | Loss: 1.1651525497436523\n",
      "Epoch 28 | Loss: 0.969400942325592\n",
      "Epoch 29 | Loss: 0.8803086280822754\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9247961821435673\n",
      "**********************\n",
      "Epoch 30 | Loss: 0.7393958568572998\n",
      "Epoch 31 | Loss: 0.63070148229599\n",
      "Epoch 32 | Loss: 0.5604259371757507\n",
      "Epoch 33 | Loss: 0.5142256617546082\n",
      "Epoch 34 | Loss: 0.47150102257728577\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9241996420759594\n",
      "**********************\n",
      "Epoch 35 | Loss: 0.4178641736507416\n",
      "Epoch 36 | Loss: 0.4000321328639984\n",
      "Epoch 37 | Loss: 0.36214080452919006\n",
      "Epoch 38 | Loss: 0.3178108334541321\n",
      "Epoch 39 | Loss: 0.29351603984832764\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9248757208192484\n",
      "**********************\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 106;\n",
       "                var nbb_unformatted_code = \"model = FancyTagger(embeddings, EMBEDDING_SIZE, HIDDEN_SIZE, len(tagset))\\nprint(model)\\nmodel.run_training(\\n    train_dataset,\\n    dev_dataset,\\n    BATCH_SIZE,\\n    vocab,\\n    tagset,\\n    lr=5e-4,\\n    num_epochs=40,\\n    eval_every=5,\\n)\";\n",
       "                var nbb_formatted_code = \"model = FancyTagger(embeddings, EMBEDDING_SIZE, HIDDEN_SIZE, len(tagset))\\nprint(model)\\nmodel.run_training(\\n    train_dataset,\\n    dev_dataset,\\n    BATCH_SIZE,\\n    vocab,\\n    tagset,\\n    lr=5e-4,\\n    num_epochs=40,\\n    eval_every=5,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert isinstance(\n",
    "    model, FancyTagger\n",
    "), \"Please assign your FancyTagger to a variable named model\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "test_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for b in range(len(test_batch_idx)):\n",
    "    logits = model.forward(test_batch_idx[b], test_batch_lens[b])\n",
    "    batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    batch_size, _ = test_batch_idx[b].shape\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        preds = batch_predictions[i]\n",
    "\n",
    "        seq_len = int(test_batch_lens[b][i])\n",
    "        for j in range(seq_len):\n",
    "            predictions.append(int(preds[j]))\n",
    "\n",
    "\n",
    "with open(\"predictions.txt\", \"w\") as f:\n",
    "    for p in predictions:\n",
    "        f.write(str(p) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 98;\n",
       "                var nbb_unformatted_code = \"# remove gpu memory\\n# del embeddings\\n# del model\\n# torch.cuda.empty_cache()\";\n",
       "                var nbb_formatted_code = \"# remove gpu memory\\n# del embeddings\\n# del model\\n# torch.cuda.empty_cache()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove gpu memory\n",
    "# del embeddings\n",
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgLM__WZw4wz"
   },
   "source": [
    "### Save Predictions\n",
    "\n",
    "When you are satisfied with your `FancyTagger`'s performance on the dev set, run the cell below to write your predictions on the test set to a text file. \n",
    "\n",
    "You can download `predictions.txt` by going to \n",
    "**View > Table of Contents > Files**\n",
    "\n",
    "Please submit this `predictions.txt` file to Gradescope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddSD3-FN9Zzp"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 107;\n",
       "                var nbb_unformatted_code = \"assert isinstance(\\n    model, FancyTagger\\n), \\\"Please assign your FancyTagger to a variable named model\\\"\\n\\nBATCH_SIZE = 32\\ntest_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)\\n\\npredictions = []\\n\\nfor b in range(len(test_batch_idx)):\\n    logits = model.forward(test_batch_idx[b], test_batch_lens[b])\\n    batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\\n\\n    batch_size, _ = test_batch_idx[b].shape\\n\\n    for i in range(batch_size):\\n        preds = batch_predictions[i]\\n\\n        seq_len = int(test_batch_lens[b][i])\\n        for j in range(seq_len):\\n            predictions.append(int(preds[j]))\\n\\n\\nwith open(\\\"predictions.txt\\\", \\\"w\\\") as f:\\n    for p in predictions:\\n        f.write(str(p) + \\\"\\\\n\\\")\";\n",
       "                var nbb_formatted_code = \"assert isinstance(\\n    model, FancyTagger\\n), \\\"Please assign your FancyTagger to a variable named model\\\"\\n\\nBATCH_SIZE = 32\\ntest_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)\\n\\npredictions = []\\n\\nfor b in range(len(test_batch_idx)):\\n    logits = model.forward(test_batch_idx[b], test_batch_lens[b])\\n    batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\\n\\n    batch_size, _ = test_batch_idx[b].shape\\n\\n    for i in range(batch_size):\\n        preds = batch_predictions[i]\\n\\n        seq_len = int(test_batch_lens[b][i])\\n        for j in range(seq_len):\\n            predictions.append(int(preds[j]))\\n\\n\\nwith open(\\\"predictions.txt\\\", \\\"w\\\") as f:\\n    for p in predictions:\\n        f.write(str(p) + \\\"\\\\n\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert isinstance(\n",
    "    model, FancyTagger\n",
    "), \"Please assign your FancyTagger to a variable named model\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "test_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for b in range(len(test_batch_idx)):\n",
    "    logits = model.forward(test_batch_idx[b], test_batch_lens[b])\n",
    "    batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    batch_size, _ = test_batch_idx[b].shape\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        preds = batch_predictions[i]\n",
    "\n",
    "        seq_len = int(test_batch_lens[b][i])\n",
    "        for j in range(seq_len):\n",
    "            predictions.append(int(preds[j]))\n",
    "\n",
    "\n",
    "with open(\"predictions.txt\", \"w\") as f:\n",
    "    for p in predictions:\n",
    "        f.write(str(p) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}