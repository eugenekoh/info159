{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dbamman/nlp20/blob/master/HW_3/HW_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2y5cwPLm6Lyw"
   },
   "source": [
    "# Homework 3: Word Embeddings\n",
    "In this homework, we will try to approximate a Skip-gram word embedding via positive pointwise mutual information (PPMI) and truncated singular value decomposition (SVD). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMnCOKC26Gzj"
   },
   "source": [
    "## The setup\n",
    "Let's import the required libraries and load the data for preparing our word vectors. We are going to load a list of movie plot summaries (http://www.cs.cmu.edu/~ark/personas/) and use that as our corpus. You do not need to modify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRKoyqtb0QL_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘plot_summaries_tokenized.csv’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/dbamman/nlp20/master/HW_3/plot_summaries_tokenized.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yWaVJn30NBk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42303 summaries.\n",
      "Example tokenized summary: ['Shlykov', 'a', 'hardworking', 'taxi', 'driver', 'and', 'Lyosha', 'a', 'saxophonist', 'develop', 'a', 'bizarre', 'lovehate', 'relationship', 'and', 'despite', 'their', 'prejudices', 'realize', 'they', 'arent', 'so', 'different', 'after', 'all']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from collections import Counter, defaultdict\n",
    "from math import log2\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads the data and returns tokenized summaries.\n",
    "    \n",
    "    :return summaries_tokenized: a list that contains tokenized summaries text\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"plot_summaries_tokenized.csv\")\n",
    "    summaries_tokenized = list(df[\"SUMMARY\"].apply(lambda text: text.split()))\n",
    "    return summaries_tokenized\n",
    "\n",
    "\n",
    "summaries = load_data()\n",
    "num_summaries = len(summaries)\n",
    "print(\"There are {} summaries.\".format(num_summaries))\n",
    "print(\"Example tokenized summary:\", summaries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ikv9DyqR7xoG"
   },
   "source": [
    "We have ~42000 summaries containing ~13000000 words. We will now proceed by creating a vocabulary and will limit its size to something computationally feasible. You may find python's collections.Counter function useful. You may not import any additional libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWP4hmGG7--v"
   },
   "source": [
    "# 1. Create Vocabulary\n",
    "We will start from creating our vocabulary. Vocabulary contains unigrams and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ksw96WHvEoJx"
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Do not modify\n",
    "###################\n",
    "min_count = (1 / 100) * len(summaries)\n",
    "max_count = (1 / 10) * len(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sum1rnZN54-V"
   },
   "outputs": [],
   "source": [
    "def creat_vocabulary(tokenized_documents, min_count, max_count):\n",
    "    \"\"\"\n",
    "    This function takes in tokenized documents and returns a\n",
    "    vocabulary and word <-> index lookup dictionary of some frequently appearing words.\n",
    "\n",
    "    :param tokenized_documents: a list of tokenized strings\n",
    "    :param min_count: minimum unigram count\n",
    "    :param max_count: maximum unigram count\n",
    "    :return vocab: a Counter where vocab[word] = count of word's occurences in all the documents\n",
    "    :return word2idx: a word -> index lookup Dictionary for words in vocab.\n",
    "    :return idx2word: a index -> word lookup Dictionary for words in vocab.\n",
    "    \"\"\"\n",
    "    # 1a. Compute unigram counts. A unigram is a single word, e.g. foo\n",
    "    vocab = Counter()\n",
    "\n",
    "    for tokens in tokenized_documents:\n",
    "        vocab.update(tokens)\n",
    "\n",
    "    # ensure that all tokens are processed\n",
    "    assert sum(vocab.values()) == sum(len(doc) for doc in tokenized_documents)\n",
    "\n",
    "    # 1b. Remove unigrams that has #(unigram) < min_count or #(unigram) > max_count\n",
    "    # to eliminate unigrams occurring very frequently or infrequently.\n",
    "    # This will limit its size to something computationally feasible.\n",
    "    print(\"%d vocabs before\" % len(vocab))\n",
    "\n",
    "    vocab = {\n",
    "        word: value for word, value in vocab.items() if min_count <= value <= max_count\n",
    "    }\n",
    "\n",
    "    print(\"%d vocabs after\" % len(vocab))\n",
    "\n",
    "    # 1c. Build word <-> index lookup for words in vocab.\n",
    "    word2idx, idx2word = {}, {}\n",
    "\n",
    "    for idx, word in enumerate(vocab.keys()):\n",
    "        word2idx[word] = idx\n",
    "        idx2word[idx] = word\n",
    "\n",
    "    assert 0 == word2idx[idx2word[0]]\n",
    "\n",
    "    return vocab, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71G0q8l_51CH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214147 vocabs before\n",
      "2750 vocabs after\n"
     ]
    }
   ],
   "source": [
    "vocab, word2idx, idx2word = creat_vocabulary(summaries, min_count, max_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0NwfhahT_tRd"
   },
   "source": [
    "# 2. Build Term-Context Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQ-tvqGE1ykI"
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Do not modify\n",
    "###################\n",
    "window_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQvXB-MZ_VqD"
   },
   "outputs": [],
   "source": [
    "def build_term_context_matrix(tokenized_documents, vocab, window_size):\n",
    "    \"\"\"\n",
    "    This function returns a `word_pair_count` Counter with each \n",
    "    word_pair_count[(w, c)] = number of times the word `c` occurs in the context of word `w`. (where `w`, `c` belong to the vocab)\n",
    "    To make it efficient, instead of building the sparse term-context matrix, \n",
    "    we will build 3 separate Counters: word_pair_count, w_count, c_count\n",
    "    You may find python's Counter useful here\n",
    "\n",
    "    :param tokenized_documents: a list of tokenized strings\n",
    "    :param vocab: vocabulary Counter\n",
    "    :param window_size: context window size\n",
    "    :return word_pair_count: a Counter where word_pair_count[(w, c)] = count of c's occurences in w's context window, i.e. #(w, c)\n",
    "    :return w_count: a Counter where w_count[w] = the number of times w occured in the documents, i.e. #(w)\n",
    "    :return c_count: a Counter where c_count[c] = the number of times c occured in the documents, i.e. #(c)\n",
    "    \"\"\"\n",
    "    word_pair_count = Counter()\n",
    "    w_count = Counter()\n",
    "    c_count = Counter()\n",
    "\n",
    "    for document in tokenized_documents:\n",
    "        for i, word in enumerate(document):\n",
    "            if word in vocab:\n",
    "                tokens = (\n",
    "                    document[i - window_size : i]\n",
    "                    + document[i + 1 : i + window_size + 1]\n",
    "                )\n",
    "\n",
    "                context_tokens = [token for token in tokens if token in vocab]\n",
    "\n",
    "                c_count.update(context_tokens)\n",
    "\n",
    "                w_count[word] += len(context_tokens)\n",
    "\n",
    "                word_pairs = [(word, context) for context in context_tokens]\n",
    "                word_pair_count.update(word_pairs)\n",
    "\n",
    "    return word_pair_count, w_count, c_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBxi0t1y2jQ_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1916213 word-context pairs\n"
     ]
    }
   ],
   "source": [
    "word_pair_count, w_count, c_count = build_term_context_matrix(\n",
    "    summaries, vocab, window_size\n",
    ")\n",
    "print(\"There are {} word-context pairs\".format(len(word_pair_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of w_count and c_count should match your number of vocab\n",
    "assert len(w_count) == len(vocab)\n",
    "assert len(c_count) == len(vocab)\n",
    "assert sum(w_count.values()) == sum(c_count.values()) == sum(word_pair_count.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeaZoasi3m5r"
   },
   "source": [
    "# 3. Build Positive Pointwise Mutual Information (PPMI) Matrix\n",
    "In this part, you will build a PPMI matrix using Scipy's Compressed Sparse Column matrix to save storage space. (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)\n",
    "\n",
    "Sparse matrix is a matrix which contains very few non-zero elements. When a sparse matrix is represented with a 2-dimensional array, we waste a lot of space to represent that matrix. In NLP application, it's quite common to use sparse matrix since the size of vocabulary is usually very large. \n",
    "\n",
    "Below is an example of how to build a sparse matrix where `data`, `row` and `col` satisfy the relationship `M[row[k], col[k]] = data[k]`.\n",
    "\n",
    "```python\n",
    ">>> row = np.array([0, 2, 2, 0, 1, 2])\n",
    ">>> col = np.array([0, 0, 1, 2, 2, 2])\n",
    ">>> data = np.array([1, 2, 3, 4, 5, 6])\n",
    ">>> M = csc_matrix((data, (row, col)))\n",
    ">>> M.toarray()\n",
    "array([[1, 0, 4],\n",
    "       [0, 0, 5],\n",
    "       [2, 3, 6]])\n",
    "```\n",
    "\n",
    "Recall that\n",
    "$$\n",
    "\\begin{gather*}\n",
    "  \\text{PMI}(w, c) = \\log_2 \\frac{P(w, c)}{P(w)P(c)} \\\\\n",
    "  \\text{PPMI}(w, c) = \\max(0, \\text{PMI}(w, c))\n",
    "\\end{gather*}\n",
    "$$\n",
    "You should use `log2` function from the math package that is alreadly imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIYharDm38G1"
   },
   "outputs": [],
   "source": [
    "def build_PPMI_matrix(word_pair_count, word_count, c_count, word2idx):\n",
    "    \"\"\"\n",
    "    This function returns a PPMI matrix represented by a csc sparse matrix.\n",
    "\n",
    "    :params word_pair_count: a Counter where word_pair_count[(w, c)] = count of c's occurences in w's context window\n",
    "    :return w_count: a Counter where w_count[w] = the number of times w occured in the documents\n",
    "    :return c_count: a Counter where c_count[c] = the number of times c occured in the documents\n",
    "    :return word2idx: a word -> index lookup Dictionary for words in vocab\n",
    "    :return PPMI: PPMI csc sparse matrix\n",
    "    \"\"\"\n",
    "    data, rows, cols = [], [], []\n",
    "    total_occurences = sum(word_pair_count.values())\n",
    "\n",
    "    for (w, c), n in word_pair_count.items():\n",
    "        prob_wc = n / total_occurences\n",
    "        prob_w = w_count[w] / total_occurences\n",
    "        prob_c = c_count[c] / total_occurences\n",
    "\n",
    "        ppmi_value = max(0, log2(prob_wc / (prob_w * prob_c)))\n",
    "        rows.append(word2idx[w])\n",
    "        cols.append(word2idx[c])\n",
    "        data.append(ppmi_value)\n",
    "\n",
    "    PPMI = csc_matrix((data, (rows, cols)))\n",
    "    return PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADuP5FPV8-XQ"
   },
   "outputs": [],
   "source": [
    "PPMI = build_PPMI_matrix(word_pair_count, w_count, c_count, word2idx)\n",
    "\n",
    "# The shape of PPMI matrix should match your number of vocab\n",
    "assert PPMI.shape == (len(vocab), len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLUHCDzN9PGF"
   },
   "source": [
    "# 4. Truncated SVD\n",
    "In this part, we will obtain a dense low-dimensional vectors via truncated (rank-k) SVD. You should use `svds` function from Sicpy that is already imported for you to obtain the SVD factorization.\n",
    "(https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEh5rynC9-UR"
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Do not modify\n",
    "###################\n",
    "rank = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLtCNz5Z9U8c"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(PPMI, rank):\n",
    "    \"\"\"\n",
    "    Reutrns the left singular vectors as word embeddings via truncated SVD\n",
    "\n",
    "    :params PPMI: PPMI csc sparse matrix\n",
    "    :params rank: number of singular values and vectors to compute\n",
    "    :return u: left sigular vectors from sprase SVD\n",
    "    :return s: the singular values from sparse SVD\n",
    "    \"\"\"\n",
    "\n",
    "    u, s, _ = svds(PPMI, rank)\n",
    "\n",
    "    return u, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmjoP5KF91O0"
   },
   "outputs": [],
   "source": [
    "embeddings, _ = get_embeddings(PPMI, rank)\n",
    "embeddings /= np.linalg.norm(\n",
    "    embeddings, axis=1, keepdims=True\n",
    ")  # Normalize embeddings matrix\n",
    "\n",
    "# The shape of the embeddings matrix should be (# vocab, rank)\n",
    "assert embeddings.shape == (len(vocab), rank)\n",
    "\n",
    "# Make sure embeddings is normalized\n",
    "assert True == np.isclose(np.linalg.norm(embeddings[0]), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQUUfS0N-Lyc"
   },
   "source": [
    "# 5. Evaluate Word Embeddings via Cosine Similarity\n",
    "\n",
    "Using cosine similarity as a measure of distance [§6.4 Jurafsky & Martin](https://web.stanford.edu/~jurafsky/slp3/6.pdf), we will now find the closest words to a certain word. We define cosine similarity as, $$cosine(\\overrightarrow{v},\\overrightarrow{w}) = \\frac{\\overrightarrow{v} \\cdot \\overrightarrow{w}}{\\vert v \\vert \\vert w \\vert}$$\n",
    "\n",
    "Please complete the function below that calculates the 'K' closest words from the vocabulary. You may not use any additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9Zf_us2AFkx"
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Do not modify\n",
    "###################\n",
    "num_neighbors = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_l55j98-NvY"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "\n",
    "def cosine_distances(matrix, vector):\n",
    "    \"\"\"\n",
    "    The function takes in a matrix and a vector (both normalized) \n",
    "    and returns the cosine distances for this vector against all others.\n",
    "    The pretrained embeddings are normalized.\n",
    "\n",
    "    :params matrix: word embeddings matrix\n",
    "    :params vector: word vector for a particular word\n",
    "    :return distances: a cosine distances vector\n",
    "    \"\"\"\n",
    "    distances = [cosine_similarity(row, vector) for row in matrix]\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def nearest_neighbors(embeddings, word, k, word2idx, idx2word):\n",
    "    \"\"\"\n",
    "    For each query word, this function returns the k closest words from the vocabulary.\n",
    "\n",
    "    :params embeddings: word embedding matrix\n",
    "    :params word: query word\n",
    "    :params k: number of cloest words to return\n",
    "    :params word2idx: a word -> index lookup dictionary\n",
    "    :params idx2word: a index -> word lookup dictionary\n",
    "    :return nearest_neighbors: a list of cloest words\n",
    "    \"\"\"\n",
    "    vector = embeddings[word2idx[word]]\n",
    "    distances = cosine_distances(embeddings, vector)\n",
    "\n",
    "    distances = np.array(distances)\n",
    "    min_k = distances.argsort()[-k - 1 : -1]\n",
    "    nearest_neighbors = [idx2word[i] for i in min_k]\n",
    "    reversed(nearest_neighbors)\n",
    "    return nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJjPuVPe_oGq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doctor ['Anna', 'Christine', 'doctors', 'priest', 'Sophie']\n",
      "zombie ['vampire', 'creatures', 'vampires', 'zombies', 'infected']\n",
      "robot ['demon', 'machine', 'weapon', 'creature', 'alien']\n",
      "eat ['stand', 'sit', 'wear', 'sleep', 'throw']\n",
      "bus ['river', 'airport', 'truck', 'boat', 'road']\n"
     ]
    }
   ],
   "source": [
    "query_words = [\"doctor\", \"zombie\", \"robot\", \"eat\", \"bus\"]\n",
    "for word in query_words:\n",
    "    print(word, nearest_neighbors(embeddings, word, num_neighbors, word2idx, idx2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfxaNuBjAiiY"
   },
   "source": [
    "# 6. Evaluate Word Embeddings via Analogous Tasks\n",
    "\n",
    "The embedding space is known to capture the semantic context of words. An example of it is $\\overrightarrow{woman} - \\overrightarrow{man} \\simeq \\overrightarrow{queen} - \\overrightarrow{king}$. Use the `cosine_distances()` function you wrote above to find such relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZQCzP-FCRb5"
   },
   "outputs": [],
   "source": [
    "def relation(embeddings, query_words, word2idx, idx2word):\n",
    "    \"\"\"\n",
    "    Takes in 3 words and returns the closest word (in terms of cosine similarity)\n",
    "    to the normalized algebraic addition of the three vectors.\n",
    "    The parameters follow this order : word_vec1 - word_vec2 ~ closest - word_vec3\n",
    "\n",
    "    :params embeddings: word embedding matrix\n",
    "    :params query_words: a list of query words in the following order: [word1, word2, word3]\n",
    "    :params word2idx: a word -> index lookup dictionary\n",
    "    :params idx2word: a index -> word lookup dictionary\n",
    "    :return closet_word: the closest word for the relation\n",
    "    \"\"\"\n",
    "    word1, word2, word3 = query_words\n",
    "    if all(word in vocab for word in query_words):\n",
    "\n",
    "        word1_vec = embeddings[word2idx[word1]]\n",
    "        word2_vec = embeddings[word2idx[word2]]\n",
    "        word3_vec = embeddings[word2idx[word3]]\n",
    "\n",
    "        target_vec = word1_vec - word2_vec + word3_vec\n",
    "        target_vec /= np.linalg.norm(target_vec)\n",
    "\n",
    "        best_distance = 0\n",
    "\n",
    "        for word in vocab:\n",
    "            if word not in query_words:\n",
    "                word_vec = embeddings[word2idx[word]]\n",
    "                distance = cosine_similarity(word_vec, target_vec)\n",
    "                if distance > best_distance:\n",
    "                    best_word = word\n",
    "                    best_distance = distance\n",
    "\n",
    "        return best_word\n",
    "\n",
    "    else:\n",
    "        missing = [w for w in query_words if w not in vocab]\n",
    "        raise Exception(\"missing {} from vocabulary\".format(\", \".join(missing)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lF3mtHMjHue-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doctor - nurse ~= Emperor - king\n",
      "robot - weapon ~= road - bus\n",
      "sing - song ~= defend - justice\n",
      "elderly - kids ~= widow - teenager\n",
      "soldier - wound ~= agent - telephone\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    [\"doctor\", \"nurse\", \"king\"],\n",
    "    [\"robot\", \"weapon\", \"bus\"],\n",
    "    [\"sing\", \"song\", \"justice\"],\n",
    "    [\"elderly\", \"kids\", \"teenager\"],\n",
    "    [\"soldier\", \"wound\", \"telephone\"],\n",
    "]\n",
    "for query in queries:\n",
    "    closet_word = relation(embeddings, query, word2idx, idx2word)\n",
    "    print(\"{} - {} ~= {} - {}\".format(query[0], query[1], closet_word, query[2]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOSIT1Z/3Qp86m2a3u5eyGX",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "HW_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
