{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dbamman/nlp20/blob/master/HW_7/HW_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cE4-tSuL0G64"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqTbFUId0MSX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "# if this cell prints \"Running on cpu\", you must switch runtime environments\n",
    "# go to Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-CW1Dvi9FYx"
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W2fVAg4f0QKz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘empatheticdialogues.tar.gz’ already there; not retrieving.\n",
      "\n",
      "empatheticdialogues/\n",
      "empatheticdialogues/test.csv\n",
      "empatheticdialogues/train.csv\n",
      "empatheticdialogues/valid.csv\n",
      "File ‘glove.6B.zip’ already there; not retrieving.\n",
      "\n",
      "Archive:  glove.6B.zip\n"
     ]
    }
   ],
   "source": [
    "#This gets the EmpatheticDialogues corpus\n",
    "!wget -nc https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz\n",
    "!tar -xvf empatheticdialogues.tar.gz\n",
    "#This gets the GLoVe embeddings, which we will use to bootstrap our model\n",
    "!wget -nc http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -n glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8shZmBg6Yxl"
   },
   "outputs": [],
   "source": [
    "PAD_INDEX = 0             # reserved for padding words\n",
    "UNKNOWN_INDEX = 1         # reserved for unknown words\n",
    "START_DECODE = 2          # special symbol to denote decoding should start\n",
    "END_DECODE = 3            # special symbol to indicate decoding is ending. This is how the model indicates the sequence is done.\n",
    "\n",
    "def read_embeddings(filename, vocab_size=10000):\n",
    "  \"\"\"\n",
    "  Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
    "  \n",
    "  Arguments:\n",
    "  - filename:     path to file\n",
    "                  automatically infers correct embedding dimension from filename\n",
    "  - vocab_size:   maximum number of embeddings to load\n",
    "\n",
    "  Returns \n",
    "  - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
    "  - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
    "  \"\"\"\n",
    "\n",
    "  # get the embedding size from the first embedding\n",
    "  with open(filename, encoding=\"utf-8\") as file:\n",
    "    word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
    "\n",
    "  vocab = {}\n",
    "  vocab[\"PAD_INDEX\"] = 0\n",
    "  vocab[\"UNKNOWN_INDEX\"] = 1\n",
    "  vocab[\"START_DECODE\"] = 2\n",
    "  vocab[\"END_DECODE\"] = 3\n",
    "\n",
    "  embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
    "\n",
    "  with open(filename, encoding=\"utf-8\") as file:\n",
    "    for idx, line in enumerate(file):\n",
    "      if idx + 4 >= vocab_size:\n",
    "        break\n",
    "\n",
    "      cols = line.rstrip().split(\" \")\n",
    "      val = np.array(cols[1:])\n",
    "      word = cols[0]\n",
    "      embeddings[idx + 4] = val\n",
    "      vocab[word] = idx + 4\n",
    "  \n",
    "  # a FloatTensor is a multidimensional matrix\n",
    "  # that contains 32-bit floats in every entry\n",
    "  # https://pytorch.org/docs/stable/tensors.html\n",
    "  return torch.FloatTensor(embeddings), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbNf4hcpnXIp"
   },
   "outputs": [],
   "source": [
    "# Let's load in a spacy tokenizer to process our conversation data\n",
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "\n",
    "class Dataset():\n",
    "  '''\n",
    "  This is a Dataset object, similar to the one used in HW4.\n",
    "\n",
    "  It serves two purposes- reading data and creating batches.\n",
    "\n",
    "  read_data():\n",
    "    Inputs: \n",
    "      filename \n",
    "      emotions_list- list of emotions to include in this dataset\n",
    "    Outputs: \n",
    "      emotions-     list of emotions associated with the dataset\n",
    "      past_turns-   list of past turns associated with the dataset.  \n",
    "                    This is input to our model.\n",
    "      responses-    list of responses associated with the dataset.  This is what \n",
    "                    we will train our model to generate.\n",
    "\n",
    "  get_batches():\n",
    "    Inputs: batch_size- size of batches we want to create \n",
    "            vocab-      our vocabulary, used to replace unknown words\n",
    "            emotset-    set of emotions to id, used to create emotion IDs\n",
    "    Outputs:  \n",
    "      batched_past_turn_idx:      indices of the words in the past turn\n",
    "      batched_past_lengths:       lengths of the past turns (since we are padding these)\n",
    "      batched_response_idx:       indices of words in the response\n",
    "      batched_past_resp_lengths:  lengths of the response (since we pad those)\n",
    "      batched_emotions:           emotions associated with the conversations\n",
    "\n",
    "  '''\n",
    "  def __init__(self, filename, emotions_list=None):\n",
    "    if emotions_list is not None:\n",
    "      self.emotions_list = ['context'] + emotions_list\n",
    "    else:\n",
    "      self.emotions_list = None\n",
    "    self.emotions, self.past_turns, self.responses = self.read_data(filename)\n",
    "\n",
    "  def read_data(self, filename):\n",
    "    past_turns = []\n",
    "    responses = []\n",
    "    emotions = []\n",
    "\n",
    "    raw_data = {}\n",
    "\n",
    "    with open(filename, encoding='utf8') as f:\n",
    "      csvreader = csv.reader(f, delimiter=',')\n",
    "      for row in csvreader:\n",
    "        convo_num = row[0]\n",
    "        emotion = row[2]  \n",
    "        utterance = row[5]\n",
    "        #if we receive an emotions_list, we need to make sure the emotion is relevant to the Dataset we create.\n",
    "        if self.emotions_list is None or emotion in self.emotions_list:\n",
    "          if convo_num not in raw_data:\n",
    "            raw_data[convo_num] = {}\n",
    "            raw_data[convo_num][\"emotion\"] = emotion\n",
    "            raw_data[convo_num][\"convo\"] = []\n",
    "          raw_data[convo_num][\"convo\"].append(utterance.replace(\"_comma_\", \",\"))\n",
    "\n",
    "      for key in raw_data:\n",
    "        for i,turn in enumerate(raw_data[key][\"convo\"]):\n",
    "          #we want to grab every other response\n",
    "          if i % 2 == 0 and i != 0:\n",
    "            emotions.append(raw_data[key][\"emotion\"])\n",
    "            past_turns.append(tokenizer(raw_data[key][\"convo\"][i-1].lower()))\n",
    "            responses.append(tokenizer(raw_data[key][\"convo\"][i].lower()))\n",
    "    \n",
    "    return emotions, past_turns, responses\n",
    "\n",
    "  def get_batches(self, batch_size, vocab, emotset):\n",
    "    # randomly shuffle the data\n",
    "    np.random.seed(159) # don't change this, for reproducibility\n",
    "    shuffle = np.random.permutation(range(len(self.past_turns)))\n",
    "    \n",
    "    #grabs the relevant data from the random permutation\n",
    "    past_turns = [self.past_turns[i] for i in shuffle]\n",
    "    emotions = [self.emotions[i] for i in shuffle]\n",
    "    responses = [self.responses[i] for i in shuffle]\n",
    "\n",
    "    #stores the id's of past_turn words\n",
    "    batched_past_turn_idx = []\n",
    "    #stores the id's of response words\n",
    "    batched_response_idx = []\n",
    "    #stores the lengths of past_turns for masking\n",
    "    batched_past_lengths = []\n",
    "    #stores the lengths of responses for masking\n",
    "    batched_past_resp_lengths = []\n",
    "    #stores the emotions associated with a batch\n",
    "    batched_emotions = []\n",
    "\n",
    "    #creates batches\n",
    "    N = len(past_turns)\n",
    "    if N % batch_size == 0:\n",
    "      num_batches = N // batch_size\n",
    "    else:\n",
    "      num_batches = N // batch_size + 1\n",
    "\n",
    "    for b in range(num_batches):\n",
    "      start = b * batch_size\n",
    "      stop = min((b+1) * batch_size, len(past_turns))\n",
    "      #calculates the max lengths of response and past turn sequences for this batch\n",
    "      max_resp_seq_len = max([len(s) for s in responses[start:stop]])\n",
    "      max_past_seq_len = max([len(s) for s in past_turns[start:stop]])\n",
    "\n",
    "      #creates the vectors for the past_turn and responses\n",
    "      past_turn_idx = np.zeros((stop-start, max_past_seq_len))\n",
    "      response_idx = np.zeros((stop-start, max_resp_seq_len + 2))\n",
    "      emotion_idx = np.empty((stop-start, 1))\n",
    "      past_lengths = np.zeros((stop-start))\n",
    "      resp_lengths = np.zeros((stop-start))\n",
    "      for i in range(start, stop):\n",
    "        #gathers the corresponding data\n",
    "        past_turn = past_turns[i]\n",
    "        response = responses[i]\n",
    "        emotion = emotions[i]\n",
    "        #gets ID for corresponding emotion\n",
    "        emotion_idx[i - start] = emotset[emotion]\n",
    "\n",
    "        #We start the response with START_DECODE to indicate to the model that decoding should start\n",
    "        response_idx[i - start][0] = START_DECODE\n",
    "\n",
    "        #this captures the lengths \n",
    "        past_lengths[i - start] = len(past_turn)\n",
    "        resp_lengths[i - start] = len(response)\n",
    "\n",
    "        #this gets the vocabulary IDs for each word in the past_turn and response\n",
    "        #UNKNOWN_INDEX is used if the word is out of vocabulary\n",
    "        for j in range(len(past_turn)):\n",
    "          if past_turn[j] in vocab:\n",
    "            past_turn_idx[i - start][j] = vocab[past_turn[j]]\n",
    "          else:\n",
    "            past_turn_idx[i - start][j] = UNKNOWN_INDEX      \n",
    "        for j in range(len(response)):\n",
    "          if response[j] in vocab:\n",
    "            response_idx[i - start][j + 1] = vocab[response[j]]\n",
    "          else:\n",
    "            response_idx[i - start][j + 1] = UNKNOWN_INDEX\n",
    "            \n",
    "        #we want to end the response with END_DECODE so the model learns to predict the end of an utterance\n",
    "        response_idx[i - start][len(response)] = END_DECODE\n",
    "      batched_past_turn_idx.append(past_turn_idx)\n",
    "      batched_response_idx.append(response_idx)\n",
    "      batched_past_lengths.append(past_lengths)\n",
    "      batched_past_resp_lengths.append(resp_lengths)\n",
    "      batched_emotions.append(emotion_idx)\n",
    "    return batched_past_turn_idx, batched_past_lengths, batched_response_idx, batched_past_resp_lengths, batched_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhjaUWnh4xZy"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function to extract a set of emotions from a dataset and associate them \n",
    "with an ID.\n",
    "\n",
    "Arguments-\n",
    "  emotion_file:   Data file we want to extract emotions from\n",
    "\n",
    "Returns:\n",
    "  emotset:        Dictionary of emotions\n",
    "'''\n",
    "def read_emotions(emotion_file):\n",
    "  emotset = {}\n",
    "  with open(emotion_file, encoding='utf8') as f:\n",
    "    csvreader = csv.reader(f, delimiter=',')\n",
    "    counter = 0\n",
    "    for row in csvreader:\n",
    "      emotion = row[2]\n",
    "      if emotion not in emotset:\n",
    "        emotset[emotion] = counter\n",
    "        counter += 1\n",
    "  \n",
    "  return emotset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5SeNJvK5Q_e"
   },
   "outputs": [],
   "source": [
    "# this loads the 10,000 most common word 300-dimensional embeddings\n",
    "vocab_size = 10000\n",
    "embeddings, vocab = read_embeddings('glove.6B.300d.txt', vocab_size)\n",
    "\n",
    "# read the files\n",
    "emotset = read_emotions('empatheticdialogues/train.csv')\n",
    "train_dataset = Dataset('empatheticdialogues/train.csv')\n",
    "dev_dataset = Dataset('empatheticdialogues/valid.csv')\n",
    "test_dataset = Dataset('empatheticdialogues/test.csv')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_batched_past_turn_idx, train_batched_past_lengths, train_batched_response_idx, train_batched_past_resp_lengths, train_batched_emotions = train_dataset.get_batches(BATCH_SIZE, vocab, emotset)\n",
    "dev_batched_past_turn_idx, dev_batched_past_lengths, dev_batched_response_idx, dev_batched_past_resp_lengths, dev_batched_emotions= dev_dataset.get_batches(BATCH_SIZE, vocab, emotset)\n",
    "test_batched_past_turn_idx, test_batched_past_lengths, test_batched_response_idx, test_batched_past_resp_lengths, test_batched_emotions= test_dataset.get_batches(BATCH_SIZE, vocab, emotset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQgF4Ni89KGL"
   },
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ik4JzhLS83N-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This class is used to create transformer-style positional encodings.  \n",
    "Reference: https://github.com/pytorch/pytorch/issues/24826\n",
    "Note: these are different than the categorical positional encodings discussed in \n",
    "class for Information Extraction.\n",
    "'''\n",
    "class TransformerPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=160):\n",
    "        super(TransformerPositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "'''\n",
    "This is the TransformerGenerator class, where the generaiton model is set up \n",
    "and the model structure is defined.\n",
    "\n",
    "Please fill in your solution where you see \"...\" \n",
    "'''\n",
    "class TransformerGenerator(nn.Module):\n",
    "  def __init__(self, embeddings, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embed = nn.Embedding.from_pretrained(embeddings, freeze=False)\n",
    "\n",
    "    self.model = nn.Transformer(d_model=ninp, \n",
    "                                nhead=nhead,\n",
    "                                num_encoder_layers=nlayers,\n",
    "                                num_decoder_layers=nlayers,\n",
    "                                dim_feedforward=nhid,\n",
    "                                dropout=dropout)\n",
    "    self.out = nn.Linear(ninp, ntoken)\n",
    "    self.pos_encoder = TransformerPositionalEncoding(ninp, dropout)\n",
    "    self.pos_decoder = TransformerPositionalEncoding(ninp, dropout)\n",
    "\n",
    "  def forward(self, past_turn, past_turn_lengths, response, response_lengths):\n",
    "\n",
    "    past_turn = torch.LongTensor(past_turn).to(device)\n",
    "    response = torch.LongTensor(response).to(device)\n",
    "    past_turn_lengths = torch.LongTensor(past_turn_lengths)\n",
    "    response_lengths = torch.LongTensor(response_lengths)\n",
    "    \n",
    "    src_masks = torch.transpose(past_turn == 0, 0, 1)\n",
    "    tgt_masks = torch.transpose(response == 0, 0, 1)\n",
    "\n",
    "    src = self.pos_encoder.forward(self.embed(past_turn))\n",
    "    tgt = self.pos_decoder.forward(self.embed(response))\n",
    "\n",
    "\n",
    "    #Ensures decoder doesn't peek at the future tokens\n",
    "    cheater_mask = self.model.generate_square_subsequent_mask(sz = len(tgt)).to(device)\n",
    "    output = self.model(src, tgt, tgt_mask = cheater_mask, src_key_padding_mask=src_masks.to(device), tgt_key_padding_mask = tgt_masks.to(device))\n",
    "    output = self.out(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "'''\n",
    "This is a function which is used to evaluate a model on a development dataset.\n",
    "This method does not update the model; rather, it is used to evaluate a model's responses\n",
    "on a dataset.\n",
    "\n",
    "Arguments:\n",
    "  model:        Model to evaluate\n",
    "  dev_dataset:  Dataset we want to evaluate with\n",
    "  batch_size:   batch size for dev dataset\n",
    "  vocab:        Vocabulary for the dataset\n",
    "  emotset:      Set of emotions for the dataset\n",
    "\n",
    "Returns:\n",
    "  avg_loss:     The average loss for the model on the dev_dataset.\n",
    "'''\n",
    "def evaluate_on_data(model, dev_dataset, batch_size, vocab,  emotset):\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=vocab[\"PAD_INDEX\"])\n",
    "    dev_batched_past_turn_idx, dev_batched_past_lengths, dev_batched_response_idx, dev_batched_resp_lengths, dev_batched_emotions= dev_dataset.get_batches(BATCH_SIZE, vocab, emotset)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "      total_loss = 0\n",
    "      for b in range(len(dev_batched_past_turn_idx)):\n",
    "          # have to transpose since model expects them in a certain format\n",
    "          src = dev_batched_past_turn_idx[b].transpose([1, 0])\n",
    "          tgt = dev_batched_response_idx[b].transpose([1, 0])\n",
    "\n",
    "          #calls the model on the current batch's input\n",
    "          logits = model.forward(src, dev_batched_past_lengths[b], tgt[:-1,:], dev_batched_resp_lengths[b])\n",
    "          # move labels to GPU memory\n",
    "          labels = torch.LongTensor(dev_batched_response_idx[b].transpose([1,0])).to(device)\n",
    "          # compute the loss with respect to true words\n",
    "          loss = loss_function(logits.view(-1, 10000), labels[1:,:].reshape(-1))\n",
    "          total_loss += loss\n",
    "      avg_loss = total_loss / float(len(dev_batched_past_turn_idx))\n",
    "      avg_loss = float(avg_loss.detach().cpu().numpy())\n",
    "      return avg_loss\n",
    "\n",
    "'''\n",
    "This is the function used to train a model.\n",
    "\n",
    "Arguments:\n",
    "  model:          model we want to train\n",
    "  train_dataset:  dataset we want to train the model with\n",
    "  dev_dataset:    dataset we want to evaluate model with during training\n",
    "  batch_size:     batch size for training\n",
    "  vocab:          vocabulary for the dataset\n",
    "  emotset:        emotion set for the dataset\n",
    "  lr:             learning rate we want to use\n",
    "  num_epochs:     epochs we want to train our model for\n",
    "  eval_every:     how often we want to evaluate on the dev dataset\n",
    "'''\n",
    "def run_training(model, train_dataset, dev_dataset, batch_size, vocab,  emotset,\n",
    "                         lr=1e-4, num_epochs=100, eval_every=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if str(device) == 'cpu':\n",
    "      print(\"Training only supported in GPU environment\")\n",
    "      return\n",
    "\n",
    "\n",
    "    # clear unreferenced data/models from GPU memory \n",
    "    torch.cuda.empty_cache()\n",
    "    # move model to GPU memory\n",
    "    model.to(device)\n",
    "\n",
    "    # set the optimizer (Adam) and loss function (CrossEnt)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=vocab[\"PAD_INDEX\"])\n",
    "\n",
    "    # batch training and dev data\n",
    "    train_batched_past_turn_idx, train_batched_past_lengths, train_batched_response_idx, train_batched_resp_lengths, train_batched_emotions = train_dataset.get_batches(BATCH_SIZE, vocab, emotset)\n",
    "    #dev_batched_past_turn_idx, dev_batched_past_lengths, dev_batched_response_idx, dev_batched_resp_lengths, dev_batched_emotions= dev_dataset.get_batches(BATCH_SIZE, vocab, emotset)\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(\"**** TRAINING *****\")\n",
    "    for i in range(num_epochs):\n",
    "      if i % eval_every == 0:\n",
    "      #  # Run on Dev data\n",
    "         dev_loss = evaluate_on_data(model, dev_dataset, batch_size, vocab,  emotset)\n",
    "         print(\"-------------------------------\")\n",
    "         print(\"Dev Loss: {}\".format(dev_loss))\n",
    "         print(\"-------------------------------\")\n",
    "\n",
    "      # sets the model in train mode\n",
    "      model.train()\n",
    "      total_loss = 0\n",
    "      for b in range(len(train_batched_past_turn_idx)):\n",
    "\n",
    "        # have to transpose since model expects them in a certain format\n",
    "        src = train_batched_past_turn_idx[b].transpose([1, 0])\n",
    "        tgt = train_batched_response_idx[b].transpose([1, 0])\n",
    "\n",
    "        #calls the model on the current batch's input\n",
    "        logits = model.forward(src, train_batched_past_lengths[b], tgt[:-1,:], train_batched_resp_lengths[b])\n",
    "        # move labels to GPU memory\n",
    "        labels = torch.LongTensor(train_batched_response_idx[b].transpose([1,0])).to(device)\n",
    "        # compute the loss with respect to true words\n",
    "        loss = loss_function(logits.view(-1, 10000), labels[1:,:].reshape(-1))\n",
    "        total_loss += loss\n",
    "\n",
    "        # propagate gradients backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # set model gradients to zero before performing next forward pass\n",
    "        model.zero_grad()\n",
    "\n",
    "      seconds_elapsed = time.time()-t0\n",
    "      mins = int(np.floor(seconds_elapsed/60))\n",
    "      secs = int(seconds_elapsed - (60*mins))\n",
    "      print(\"Epoch {} | Train Loss: {} | Time: {} mins, {} secs\".format(i, total_loss / float(len(train_batched_past_turn_idx)),mins,secs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4e1LiLFn-mZ4"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  \"\"\"\n",
    "  Sets random seeds and sets model in deterministic\n",
    "  training mode. Ensures reproducible results\n",
    "  \"\"\"\n",
    "  torch.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRELt-cKm5RI"
   },
   "outputs": [],
   "source": [
    "# sets the random seed – DO NOT change this\n",
    "# this ensures deterministic results that are comparable with the staff values\n",
    "set_seed(159)\n",
    "\n",
    "'''Do NOT change these parameters'''\n",
    "#Number of vocabulary words we have\n",
    "VOCAB_SIZE = 10000\n",
    "#Size of our word embeddings.  We embed each word before passing into the transformer layer,\n",
    "#so the transformer needs to know how large these embeddings will be\n",
    "NINP = 300\n",
    "#The number of heads we want our transformer model to have\n",
    "NHEAD = 6\n",
    "#The size of hidden dimensions we want our transformer to have\n",
    "NHIDDEN = 200\n",
    "#The number of layers we want our transformer to have\n",
    "NLAYERS = 1\n",
    "#Dropout rate\n",
    "DROPOUT = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsv_Rtks35gS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNOTE: do NOT run this cell if you are loading a pre-trained model.\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "NOTE: do NOT run this cell if you are loading a pre-trained model.\n",
    "'''\n",
    "\n",
    "# #This is the call which initializes the model\n",
    "# model = TransformerGenerator(embeddings, VOCAB_SIZE, NINP, NHEAD, NHIDDEN, NLAYERS, DROPOUT)\n",
    "\n",
    "# # This call trains the model.  If you have implemented Q1 correctly, the loss should decrease from ~5.6 to ~3.8\n",
    "# # Sanity check: if this function fails, your Q1 code is probably incorrect. \n",
    "# run_training(model, train_dataset, dev_dataset, BATCH_SIZE, vocab, emotset, \n",
    "#                    lr=1e-4, num_epochs=25, eval_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRm_VBOrOHPe"
   },
   "source": [
    "**If you want to save your trained model so you don't have to train it again for #2, please run the following cell.  You will need to download the model file and import this to Colab the next time you'd like to load it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ss4X5RHFxQIu"
   },
   "outputs": [],
   "source": [
    "# #Now, let's save this model so you won't have to run it again for #2.\n",
    "#  torch.save(model.state_dict(), \"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vh4jVENRxfRA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerGenerator(\n",
       "  (embed): Embedding(10000, 300)\n",
       "  (model): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=200, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=200, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "          (dropout3): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=300, out_features=10000, bias=True)\n",
       "  (pos_encoder): TransformerPositionalEncoding(\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (pos_decoder): TransformerPositionalEncoding(\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE: if you have a saved version of the model, un-comment and run this code to load your model back in.\n",
    "model = TransformerGenerator(embeddings, VOCAB_SIZE, NINP, NHEAD, NHIDDEN, NLAYERS, DROPOUT)\n",
    "model.load_state_dict(torch.load(\"./model\"))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfRZScz17wLC"
   },
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qZcBsYZgAl89"
   },
   "outputs": [],
   "source": [
    "def id2string(vocab, response):\n",
    "\n",
    "    \"\"\"\n",
    "    id2string function, takes a vocabulary and response and translates the response to a list of strings using the vocab.\n",
    "    \n",
    "    Arguments:\n",
    "    - vocab:        vocabulary, keys are strings and values are IDs\n",
    "    - response:     list of IDs we want to translate\n",
    "\n",
    "    Returns \n",
    "    - str_response:   list of strings containing the words represented by the response's IDs\n",
    "    \"\"\"\n",
    "    vocab_inverse = {v: k for k, v in vocab.items()}\n",
    "    str_response = [vocab_inverse[int(idx)] for idx in response]\n",
    "    \n",
    "    return str_response\n",
    "\n",
    "    \n",
    "    \n",
    "def decode(model, prev_turn, prev_lengths, vocab, max_len, random_top_k=False):#, batched_resps, batched_resp_lens, vocab, max_len):\n",
    "    \"\"\"\n",
    "    Decode function, takes a trained model and past_turn and returns the model's generated response\n",
    "    \n",
    "    Arguments:\n",
    "    - model:        trained model that we want to evaluate\n",
    "    - prev_turn:    The previous turn we want to generate a model response for\n",
    "    - prev_lengths: The length of the prev_turn\n",
    "    - max_len:      The maximum decoded sequence length\n",
    "    - random_top_k: Flag specifying whether to use topK decoding\n",
    "\n",
    "    Returns \n",
    "    - id2string(vocab, prediction):   list of strings indicating the words produced \n",
    "                                      by the model, calculated using id2string helper function. \n",
    "    \"\"\"\n",
    "    prediction = [vocab['START_DECODE']]\n",
    "    prev_turn = [[tok] for tok in prev_turn[0] ]\n",
    "    response = [[tok] for tok in prediction]\n",
    "    response_lengths = [[max_len]]\n",
    "    \n",
    "    for i in range(1, max_len + 1):\n",
    "\n",
    "        output = model.forward(prev_turn, prev_lengths, response, response_lengths)\n",
    "        \n",
    "        logits = output[-1].squeeze()\n",
    "        \n",
    "        # predict\n",
    "        if random_top_k:\n",
    "            _, top_k = torch.topk(logits, 5)\n",
    "            word_idx = np.random.choice(top_k.cpu())\n",
    "        else:\n",
    "            _, word_idx = logits.max(0)\n",
    "        \n",
    "        response.append([int(word_idx)])\n",
    "        \n",
    "        if word_idx == 3:\n",
    "            break\n",
    "        \n",
    "    prediction = [x[0] for x in response]\n",
    "    return id2string(vocab, prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZRfpe9nzrgW"
   },
   "source": [
    "### Let's test the base decode() function by evaluating it with some model responses to dev set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emS1i7NB806j"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function runs a trained model to respond to random examples from a dataset\n",
    "\n",
    "Arguments:\n",
    "  dataset:      Dataset we want to evaluate the model with\n",
    "  model:        The model we want to evaluate\n",
    "  random_top_k: Whether we want to use topk decoding\n",
    "'''\n",
    "\n",
    "def generate_5_responses(dataset, model, random_top_k=False):\n",
    "  dev_batched_past_turn_idx, dev_batched_past_lengths, dev_batched_response_idx, dev_batched_resp_lengths, dev_batched_emotions= dataset.get_batches(BATCH_SIZE, vocab, emotset)\n",
    "  for _ in range(5):\n",
    "    rand_batch = random.randint(0, len(dev_batched_past_turn_idx)-1)\n",
    "    rand_item = random.randint(0, len(dev_batched_past_turn_idx[0])-1)\n",
    "    print(\"Past response: \")\n",
    "    print(id2string(vocab, [x for x in dev_batched_past_turn_idx[rand_batch][rand_item] if x != PAD_INDEX]))\n",
    "    print(\"Model Response: \")\n",
    "    model_resp = decode(model=model, prev_turn=[dev_batched_past_turn_idx[rand_batch][rand_item]], prev_lengths=[dev_batched_past_lengths[rand_batch][rand_item]], vocab=vocab, max_len=20, random_top_k=random_top_k)\n",
    "    print(model_resp)\n",
    "    print(\"Gold Response: \")\n",
    "    print(id2string(vocab, [x for x in dev_batched_response_idx[rand_batch][rand_item][1:] if x != PAD_INDEX]))\n",
    "    print()\n",
    "    print(\"---------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5-as-xwYkXu",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past response: \n",
      "['oh', 'man', ',', 'that', \"'s\", 'so', 'nerve', '-', 'UNKNOWN_INDEX', '.', 'do', 'you', 'think', 'you', 'guys', 'will', 'be', 'a', 'good', 'fit', '?']\n",
      "Model Response: \n",
      "['START_DECODE', 'i', 'am', 'so', 'UNKNOWN_INDEX', '.', 'i', \"'m\", 'not', 'sure', 'if', 'i', \"'m\", 'not', 'sure', 'i', \"'m\", 'not', 'sure', 'if', 'i']\n",
      "Gold Response: \n",
      "['he', 'spent', 'some', 'time', 'laying', 'out', 'his', 'beliefs', 'and', 'his', 'expectations', ',', 'and', 'i', 'have', 'to', 'say', ',', 'i', 'agreed', 'with', 'all', 'but', 'one', 'of', 'them', '.', 'UNKNOWN_INDEX', 'that', \"'s\", 'pretty', 'good', '.', 'UNKNOWN_INDEX', 'he', 'spent', 'a', 'lot', 'of', 'time', 'watching', 'the', 'rest', 'of', 'us', 'UNKNOWN_INDEX', 'with', 'each', 'other', ',', 'so', 'i', 'think', 'he', 'is', 'trying', 'to', 'feel', 'us', 'out', 'without', 'making', 'UNKNOWN_INDEX', 'yet', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['yes', ',', 'it', 'is', 'important', '.', 'what', 'happened', '?']\n",
      "Model Response: \n",
      "['START_DECODE', 'i', 'was', \"n't\", 'expecting', 'it', '.', 'i', 'was', \"n't\", 'expecting', 'it', 'END_DECODE']\n",
      "Gold Response: \n",
      "['nothing', 'happened', 'on', 'my', 'end', ',', 'but', 'my', 'mate', 'used', 'to', 'be', 'a', 'UNKNOWN_INDEX', 'UNKNOWN_INDEX', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['you', 'have', 'eyes', 'too']\n",
      "Model Response: \n",
      "['START_DECODE', 'i', 'am', 'so', 'UNKNOWN_INDEX', '.', 'i', \"'m\", 'not', 'sure', 'i', \"'m\", 'not', 'sure', 'END_DECODE']\n",
      "Gold Response: \n",
      "['yes', 'i', 'do', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['UNKNOWN_INDEX', 'that', \"'s\", 'rough', '...', 'did', 'you', 'talk', 'to', 'them', 'about', 'it']\n",
      "Model Response: \n",
      "['START_DECODE', 'i', 'did', \"n't\", 'know', '.', 'i', 'was', 'so', 'UNKNOWN_INDEX', 'END_DECODE']\n",
      "Gold Response: \n",
      "['no', ',', 'sometimes', 'i', 'feel', 'like', 'they', 'just', 'do', \"n't\", 'understand', 'me', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['oh', 'that', 'sounds', 'so', 'UNKNOWN_INDEX', '!']\n",
      "Model Response: \n",
      "['START_DECODE', 'i', 'know', '.', 'i', 'was', 'so', 'UNKNOWN_INDEX', 'END_DECODE']\n",
      "Gold Response: \n",
      "['it', 'was', 'very', 'UNKNOWN_INDEX', ',', 'i', \"'m\", 'UNKNOWN_INDEX', 'of', 'UNKNOWN_INDEX', '.', 'what', 'are', 'you', 'afraid', 'of', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_5_responses(dev_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M2R6gNzvz9zS"
   },
   "source": [
    "### Now, let's compare the base decode() method to the Random-Top5 decoding method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmqcOyzR0ITO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past response: \n",
      "['oh', 'wonderful', '!', 'one', 'of', 'the', 'best', 'fields', 'to', 'be', 'in', 'right', 'now']\n",
      "Model Response: \n",
      "['START_DECODE', 'yeah', ',', 'it', \"'s\", 'been', 'UNKNOWN_INDEX', 'and', 'it', 'makes', 'you', 'have', 'UNKNOWN_INDEX', 'END_DECODE']\n",
      "Gold Response: \n",
      "['i', 'love', 'it', '.', 'just', 'ten', 'more', 'classes', 'to', 'go', 'until', 'i', 'finish', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['oh', 'no', 'UNKNOWN_INDEX', 'that', 'is', 'a', 'bit', 'UNKNOWN_INDEX', '.', 'i', 'wish', 'the', 'best', 'for', 'you', '!']\n",
      "Model Response: \n",
      "['START_DECODE', 'thank', 'you', '!', '!', 'it', 'makes', 'me', 'UNKNOWN_INDEX', 'UNKNOWN_INDEX', 'UNKNOWN_INDEX', ',', 'i', 'think', 'i', 'will', 'be', 'so', 'END_DECODE']\n",
      "Gold Response: \n",
      "['thanks', '!', 'i', 'hope', 'more', 'work', 'will', 'come', 'soon', '.', 'are', 'you', 'doing', 'well', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['i', 'am', 'the', 'same', 'way', ',', 'i', 'barely', 'make', 'anything', 'compared', 'to', 'a', 'lot', 'of', 'people', 'i', 'know']\n",
      "Model Response: \n",
      "['START_DECODE', 'i', 'hope', 'i', 'do', '.', 'they', 'do', 'not', 'want', 'to', 'see', 'how', 'you', \"'re\", 'going', 'out', 'to', 'see', 'that', 'way']\n",
      "Gold Response: \n",
      "['yeah', ',', 'and', 'it', 'seems', 'so', 'hard', 'to', 'make', 'money', 'these', 'days', ',', 'at', 'least', 'for', 'me', '.', 'maybe', 'i', 'should', 'move', 'to', 'the', 'city', 'where', 'there', 'are', 'more', 'opportunities', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['even', 'worse', '.', 'i', \"'m\", 'so', 'sorry', '.', 'what', 'did', 'you', 'do', 'instead', '?']\n",
      "Model Response: \n",
      "['START_DECODE', 'UNKNOWN_INDEX', '.', 'i', 'did', \"n't\", 'get', 'to', 'be', 'END_DECODE']\n",
      "Gold Response: \n",
      "['a', 'bowl', 'of', 'dry', 'UNKNOWN_INDEX', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['i', \"'ve\", 'had', 'similar', 'experiences', '.', 'UNKNOWN_INDEX', 'what', 'do', 'you', 'end', 'up', 'doing', '?']\n",
      "Model Response: \n",
      "['START_DECODE', 'well', 'i', 'do', 'nt', 'think', 'about', 'my', 'life', 'and', 'is', 'a', 'little', 'UNKNOWN_INDEX', '.', 'UNKNOWN_INDEX', 'i', 'am', 'not', 'END_DECODE']\n",
      "Gold Response: \n",
      "['i', 'write', 'up', 'UNKNOWN_INDEX', 'for', 'development', '.', 'it', 'just', 'gets', 'UNKNOWN_INDEX', 'when', 'they', 'start', 'to', 'demand', 'that', 'i', 'fix', 'it', '.', 'UNKNOWN_INDEX', 'i', 'might', 'be', 'good', 'at', 'my', 'job', 'but', 'not', 'good', 'enough', 'to', 'do', 'a', 'code', 'change', 'on', 'the', 'fly', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_5_responses(dev_dataset, model, random_top_k=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCkPZwaP7yLT"
   },
   "source": [
    "# Effect of Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4dWUtkoJDgEP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here is where we divide up the emotions in the emotset into positive and negative emotions, of equal sizes.\n",
    "positive_emotions = ['anticipating', 'caring', 'confident', 'content', 'excited', 'faithful', 'grateful', 'hopeful', 'impressed', 'joyful', 'nostalgic', 'prepared', 'proud', 'sentimental','surprised','trusting']\n",
    "negative_emotions = ['afraid', 'angry', 'annoyed', 'anxious', 'apprehensive', 'ashamed','devastated','disappointed','disgusted', 'embarrassed','furious','guilty','jealous','lonely','sad','terrified']\n",
    "len(positive_emotions), len(negative_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CKMtZHLfDgSP"
   },
   "outputs": [],
   "source": [
    "# creates the positive and negative datasets, specifying the relevant emotions_list\n",
    "train_dataset_positive = Dataset('empatheticdialogues/train.csv', emotions_list=positive_emotions)\n",
    "dev_dataset_positive = Dataset('empatheticdialogues/valid.csv', emotions_list=positive_emotions)\n",
    "test_dataset_positive = Dataset('empatheticdialogues/test.csv', emotions_list=positive_emotions)\n",
    "\n",
    "train_dataset_negative = Dataset('empatheticdialogues/train.csv', emotions_list=negative_emotions)\n",
    "dev_dataset_negative = Dataset('empatheticdialogues/valid.csv', emotions_list=negative_emotions)\n",
    "test_dataset_negative = Dataset('empatheticdialogues/test.csv', emotions_list=negative_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLyS6YVuLO4Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive model\n",
      "**** TRAINING *****\n",
      "-------------------------------\n",
      "Dev Loss: 9.343725204467773\n",
      "-------------------------------\n",
      "Epoch 0 | Train Loss: 6.205113887786865 | Time: 0 mins, 19 secs\n",
      "Epoch 1 | Train Loss: 5.527551174163818 | Time: 0 mins, 37 secs\n",
      "Epoch 2 | Train Loss: 5.396483421325684 | Time: 0 mins, 56 secs\n",
      "Epoch 3 | Train Loss: 5.207787990570068 | Time: 1 mins, 14 secs\n",
      "Epoch 4 | Train Loss: 5.072014331817627 | Time: 1 mins, 33 secs\n",
      "-------------------------------\n",
      "Dev Loss: 5.119721412658691\n",
      "-------------------------------\n",
      "Epoch 5 | Train Loss: 4.96657133102417 | Time: 1 mins, 52 secs\n",
      "Epoch 6 | Train Loss: 4.879489421844482 | Time: 2 mins, 11 secs\n",
      "Epoch 7 | Train Loss: 4.809825897216797 | Time: 2 mins, 29 secs\n",
      "Epoch 8 | Train Loss: 4.749461650848389 | Time: 2 mins, 48 secs\n",
      "Epoch 9 | Train Loss: 4.695068359375 | Time: 3 mins, 7 secs\n",
      "-------------------------------\n",
      "Dev Loss: 4.798073768615723\n",
      "-------------------------------\n",
      "Epoch 10 | Train Loss: 4.646487236022949 | Time: 3 mins, 26 secs\n",
      "Epoch 11 | Train Loss: 4.605850696563721 | Time: 3 mins, 45 secs\n",
      "Epoch 12 | Train Loss: 4.5673980712890625 | Time: 4 mins, 4 secs\n",
      "Epoch 13 | Train Loss: 4.532947540283203 | Time: 4 mins, 23 secs\n",
      "Epoch 14 | Train Loss: 4.504837512969971 | Time: 4 mins, 41 secs\n",
      "-------------------------------\n",
      "Dev Loss: 4.662332057952881\n",
      "-------------------------------\n",
      "Epoch 15 | Train Loss: 4.478322505950928 | Time: 5 mins, 1 secs\n",
      "Epoch 16 | Train Loss: 4.447730541229248 | Time: 5 mins, 20 secs\n",
      "Epoch 17 | Train Loss: 4.4266486167907715 | Time: 5 mins, 38 secs\n",
      "Epoch 18 | Train Loss: 4.403996467590332 | Time: 5 mins, 57 secs\n",
      "Epoch 19 | Train Loss: 4.383289337158203 | Time: 6 mins, 15 secs\n",
      "-------------------------------\n",
      "Dev Loss: 4.594818115234375\n",
      "-------------------------------\n",
      "Epoch 20 | Train Loss: 4.364870071411133 | Time: 6 mins, 35 secs\n",
      "Epoch 21 | Train Loss: 4.343915939331055 | Time: 6 mins, 53 secs\n",
      "Epoch 22 | Train Loss: 4.3238372802734375 | Time: 7 mins, 11 secs\n",
      "Epoch 23 | Train Loss: 4.304798126220703 | Time: 7 mins, 30 secs\n",
      "Epoch 24 | Train Loss: 4.2895426750183105 | Time: 7 mins, 49 secs\n",
      "Negative model\n",
      "**** TRAINING *****\n",
      "-------------------------------\n",
      "Dev Loss: 9.364490509033203\n",
      "-------------------------------\n",
      "Epoch 0 | Train Loss: 6.132111072540283 | Time: 0 mins, 20 secs\n",
      "Epoch 1 | Train Loss: 5.4318647384643555 | Time: 0 mins, 40 secs\n",
      "Epoch 2 | Train Loss: 5.268693447113037 | Time: 0 mins, 59 secs\n",
      "Epoch 3 | Train Loss: 5.09381628036499 | Time: 1 mins, 19 secs\n",
      "Epoch 4 | Train Loss: 4.960585117340088 | Time: 1 mins, 39 secs\n",
      "-------------------------------\n",
      "Dev Loss: 4.99207067489624\n",
      "-------------------------------\n",
      "Epoch 5 | Train Loss: 4.85889196395874 | Time: 1 mins, 59 secs\n",
      "Epoch 6 | Train Loss: 4.770734786987305 | Time: 2 mins, 19 secs\n",
      "Epoch 7 | Train Loss: 4.702010631561279 | Time: 2 mins, 39 secs\n",
      "Epoch 8 | Train Loss: 4.639385223388672 | Time: 2 mins, 58 secs\n",
      "Epoch 9 | Train Loss: 4.587775230407715 | Time: 3 mins, 18 secs\n",
      "-------------------------------\n",
      "Dev Loss: 4.739933490753174\n",
      "-------------------------------\n",
      "Epoch 10 | Train Loss: 4.540408134460449 | Time: 3 mins, 39 secs\n",
      "Epoch 11 | Train Loss: 4.502945899963379 | Time: 3 mins, 59 secs\n",
      "Epoch 12 | Train Loss: 4.466370105743408 | Time: 4 mins, 19 secs\n",
      "Epoch 13 | Train Loss: 4.4357590675354 | Time: 4 mins, 38 secs\n",
      "Epoch 14 | Train Loss: 4.41035270690918 | Time: 4 mins, 57 secs\n",
      "-------------------------------\n",
      "Dev Loss: 4.633687496185303\n",
      "-------------------------------\n",
      "Epoch 15 | Train Loss: 4.382290840148926 | Time: 5 mins, 18 secs\n",
      "Epoch 16 | Train Loss: 4.358410358428955 | Time: 5 mins, 38 secs\n",
      "Epoch 17 | Train Loss: 4.338099479675293 | Time: 5 mins, 58 secs\n",
      "Epoch 18 | Train Loss: 4.315113544464111 | Time: 6 mins, 17 secs\n",
      "Epoch 19 | Train Loss: 4.294272422790527 | Time: 6 mins, 37 secs\n",
      "-------------------------------\n",
      "Dev Loss: 4.540405750274658\n",
      "-------------------------------\n",
      "Epoch 20 | Train Loss: 4.277548789978027 | Time: 6 mins, 57 secs\n",
      "Epoch 21 | Train Loss: 4.259275436401367 | Time: 7 mins, 17 secs\n",
      "Epoch 22 | Train Loss: 4.243080139160156 | Time: 7 mins, 36 secs\n",
      "Epoch 23 | Train Loss: 4.227299213409424 | Time: 7 mins, 56 secs\n",
      "Epoch 24 | Train Loss: 4.214248180389404 | Time: 8 mins, 16 secs\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "NOTE: these models train quicker than the Q1 model, as they are trained on 1/2 of the data.\n",
    "'''\n",
    "\n",
    "#we need to reset these embeddings or they will be shared among all 3 models.\n",
    "positive_embeddings, _ = read_embeddings('glove.6B.300d.txt', vocab_size)\n",
    "negative_embeddings, _ = read_embeddings('glove.6B.300d.txt', vocab_size)\n",
    "\n",
    "#This is the call which initializes the model\n",
    "positive_model = TransformerGenerator(positive_embeddings, VOCAB_SIZE, NINP, NHEAD, NHIDDEN, NLAYERS, DROPOUT)\n",
    "negative_model = TransformerGenerator(negative_embeddings, VOCAB_SIZE, NINP, NHEAD, NHIDDEN, NLAYERS, DROPOUT)\n",
    "\n",
    "print(\"Positive model\")\n",
    "run_training(positive_model, train_dataset_positive, dev_dataset_positive, BATCH_SIZE, vocab, emotset, \n",
    "                   lr=1e-4, num_epochs=25, eval_every=5)\n",
    "print(\"Negative model\")\n",
    "run_training(negative_model, train_dataset_negative, dev_dataset_negative, BATCH_SIZE, vocab, emotset, \n",
    "                   lr=1e-4, num_epochs=25, eval_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmNYBkRWynJL"
   },
   "source": [
    "### Let's decode the *positive* model on some *positive* data to see the types of responses it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZxhvkxSu8u_0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past response: \n",
      "['oh', 'man', 'i', 'would', 'have', 'UNKNOWN_INDEX', 'i', 'hate', 'being', 'up', 'so', 'high', '.', 'he', 'sounds', 'really', 'brave', '!']\n",
      "Model Response: \n",
      "['START_DECODE', 'it', 'does', 'you', 'is', 'UNKNOWN_INDEX', '.', 'it', 'was', \"n't\", 'UNKNOWN_INDEX', '.', 'UNKNOWN_INDEX', '.', 'but', 'it', 'was', 'great', 'END_DECODE']\n",
      "Gold Response: \n",
      "['i', 'know', ',', 'he', 'usually', 'never', 'goes', 'on', 'dangerous', 'rides', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['yes', ',', 'what', 'about', 'it', '?']\n",
      "Model Response: \n",
      "['START_DECODE', 'it', 'UNKNOWN_INDEX', 'and', 'my', 'old', ',', 'i', 'have', 'a', 'new', 'UNKNOWN_INDEX', 'UNKNOWN_INDEX', ',', 'i', 'have', 'it', 'a', 'END_DECODE']\n",
      "Gold Response: \n",
      "['so', 'i', 'guess', 'i', 'just', 'dropped', 'it', ',', 'and', 'a', 'very', 'old', 'man', ',', 'picked', 'it', 'up', 'and', 'gave', 'it', 'back', 'to', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['i', 'bet', 'it', 'was', \"n't\", 'UNKNOWN_INDEX', '!', 'they', 'would', \"n't\", 'even', 'give', 'you', 'a', 'cup', 'of', 'water', 'for', 'free', '.']\n",
      "Model Response: \n",
      "['START_DECODE', 'UNKNOWN_INDEX', 'UNKNOWN_INDEX', 'i', 'did', ',', 'but', 'it', 'so', 'many', 'people', 'are', 'so', 'i', 'have', 'it', 'was', 'so', 'much', 'END_DECODE']\n",
      "Gold Response: \n",
      "['that', \"'s\", 'so', 'funny', '!', 'i', 'was', 'so', 'surprised', 'too', 'but', 'really', 'happy', '.', 'i', 'might', 'even', 'pay', 'for', 'the', 'services', 'when', 'my', 'free', 'year', 'is', 'up', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['oh', 'no', '!', 'UNKNOWN_INDEX', 'what', 'has', 'you', 'scared', '?']\n",
      "Model Response: \n",
      "['START_DECODE', 'UNKNOWN_INDEX', 'and', 'she', \"'s\", 'a', 'great', 'END_DECODE']\n",
      "Gold Response: \n",
      "['it', \"'s\", 'all', 'new', 'to', 'me', ',', 'i', \"'ve\", 'never', 'done', 'it', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['really', 'depends', 'on', 'the', 'person', 'i', 'think', '.', 'you', 'ca', \"n't\", 'have', 'good', 'people', 'without', 'bad', 'people', '.']\n",
      "Model Response: \n",
      "['START_DECODE', 'yeah', 'it', 'is', '!', '!', 'UNKNOWN_INDEX', 'it', 'will', 'be', 'UNKNOWN_INDEX', 'UNKNOWN_INDEX', 'for', 'my', 'wife', 'a', 'lot', 'and', 'i', 'have', \"n't\"]\n",
      "Gold Response: \n",
      "['yes', 'that', 'is', 'true', '.', 'the', 'UNKNOWN_INDEX', 'and', 'the', 'yang', '.', 'but', 'i', 'think', 'most', 'people', 'are', 'good', 'though', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_5_responses(dataset=dev_dataset_positive, model=positive_model, random_top_k=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCGDPLI5y02x"
   },
   "source": [
    "### Let's decode the *negative* model on some *negative* data to see the types of responses it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JpRmXr7NyvQd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past response: \n",
      "['i', 'have', 'considered', 'it', 'myself', 'but', 'feel', 'the', 'same', 'as', 'you', '.', 'will', 'you', 'go', 'back', 'again', '?']\n",
      "Model Response: \n",
      "['START_DECODE', 'no', 'it', 'is', 'so', 'much', ',', 'so', 'much', 'UNKNOWN_INDEX', 'of', 'it', 'END_DECODE']\n",
      "Gold Response: \n",
      "['i', 'might', '.', 'it', 'just', 'makes', 'me', 'a', 'little', 'UNKNOWN_INDEX', 'having', 'to', 'share', 'personal', 'information', 'with', 'a', 'total', 'UNKNOWN_INDEX', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['hopefully', 'it', 'will', 'stop', 'soon', '.', 'that', \"'s\", 'the', 'worst', 'having', 'to', 'get', 'creative', 'with', 'the', 'little', 'food', 'you', 'have', 'on', 'hand', '.']\n",
      "Model Response: \n",
      "['START_DECODE', 'yeah', 'i', \"'m\", 'UNKNOWN_INDEX', 'at', 'a', 'new', 'little', 'END_DECODE']\n",
      "Gold Response: \n",
      "['actually', 'i', 'have', 'no', 'food', 'and', 'i', 'am', 'UNKNOWN_INDEX', '.', 'maybe', 'i', 'should', 'order', 'a', 'pizza', '.', 'but', 'then', 'the', 'delivery', 'guy', 'will', 'have', 'the', 'same', 'problem', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['what', 'happened', '?', '?', 'did', 'you', 'guys', 'went', 'UNKNOWN_INDEX', 'or', 'something', '?']\n",
      "Model Response: \n",
      "['START_DECODE', 'it', 'did', 'not', 'even', 'know', '.', 'he', 'did', 'nt', 'like', 'my', 'UNKNOWN_INDEX', '.', 'i', 'was', 'not', 'the', 'UNKNOWN_INDEX', 'UNKNOWN_INDEX', 'END_DECODE']\n",
      "Gold Response: \n",
      "['my', 'grandmother', 'fell', 'ill', ',', 'and', 'passed', 'away', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['UNKNOWN_INDEX', ',', 'that', \"'s\", 'a', 'tough', 'one', 'and', 'i', 'can', 'definitely', 'feel', 'your', 'pain', 'on', 'the', 'job', 'market', '.', 'i', 'think', 'either', 'network', 'your', 'way', 'to', 'a', 'higher', 'pay', 'job', '(', 'depends', 'on', 'connections', ')', 'or', 'possibly', 'go', 'back', 'to', 'school', 'to', 'a', 'doctorate', 'or', 'certain', 'specialist', 'degrees', '.']\n",
      "Model Response: \n",
      "['START_DECODE', 'i', 'know', 'right', '.', 'UNKNOWN_INDEX', 'i', 'have', 'to', 'a', 'little', '.', 'UNKNOWN_INDEX', 'END_DECODE']\n",
      "Gold Response: \n",
      "['UNKNOWN_INDEX', '?', 'excuse', 'me', '?', 'do', 'you', 'know', 'people', 'earn', 'a', 'lot', 'from', 'computer', 'science', ',', 'it', 'is', 'not', 'my', 'degree', 'for', 'sure', ',', 'it', \"'s\", 'this', 'corrupt', 'world', 'we', 'live', 'in', 'that', \"'s\", 'the', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Past response: \n",
      "['oh', 'no', '!', 'i', 'UNKNOWN_INDEX', 'i', 'have', 'done', 'that', 'to', 'my', 'kids', '!']\n",
      "Model Response: \n",
      "['START_DECODE', 'UNKNOWN_INDEX', '.', 'he', 'was', 'just', 'the', 'time', 'and', 'they', 'do', \"n't\", 'have', 'to', 'UNKNOWN_INDEX', 'them', 'and', 'i', 'do', 'not', 'have']\n",
      "Gold Response: \n",
      "['i', 'have', 'too', 'now', 'that', 'i', 'have', 'kids', 'of', 'my', 'own', '!', '!', 'i', 'think', 'that', 'the', 'use', 'by', 'dates', 'on', 'food', 'are', 'more', 'of', 'a', 'UNKNOWN_INDEX', ',', 'and', 'so', 'far', 'they', 'have', \"n't\", 'got', 'ill', 'END_DECODE']\n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_5_responses(dataset=dev_dataset_negative, model=negative_model, random_top_k=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZCCUs8hy30V"
   },
   "source": [
    "### Let's decode *both* models on some *positive* data to see the types of responses it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    END = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Znget6A-yvZd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPast response: \u001b[0m\n",
      "i bet she was so excited ! ! did she love her presents ?\n",
      "\u001b[1mPOSITIVE model Response: \u001b[0m\n",
      "START_DECODE it does she does she was UNKNOWN_INDEX . UNKNOWN_INDEX she was a little lot . i did n't know it\n",
      "\u001b[1mNEGATIVE model Response: \u001b[0m\n",
      "START_DECODE yes it did nt . UNKNOWN_INDEX she is UNKNOWN_INDEX , she had a good time UNKNOWN_INDEX she is END_DECODE\n",
      "\u001b[1mGold Response: \u001b[0m\n",
      "yeah she did , but i guess i was looking forward to it more than she was END_DECODE\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "\u001b[1mPast response: \u001b[0m\n",
      "nice ! you must be pretty confident she will say yes .\n",
      "\u001b[1mPOSITIVE model Response: \u001b[0m\n",
      "START_DECODE i think i 'm very UNKNOWN_INDEX she 's a lot to go to go to be a great time she\n",
      "\u001b[1mNEGATIVE model Response: \u001b[0m\n",
      "START_DECODE i have been so i do nt think she has UNKNOWN_INDEX to do it her to do it END_DECODE\n",
      "\u001b[1mGold Response: \u001b[0m\n",
      "yes i agree . i feel that way about it . you know me too well END_DECODE\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "\u001b[1mPast response: \u001b[0m\n",
      "oh how exciting ! did you take any pictures ?\n",
      "\u001b[1mPOSITIVE model Response: \u001b[0m\n",
      "START_DECODE no , it went back to go out to see me and UNKNOWN_INDEX it END_DECODE\n",
      "\u001b[1mNEGATIVE model Response: \u001b[0m\n",
      "START_DECODE UNKNOWN_INDEX it is so . it 's just just the time . it was so gross . i was n't\n",
      "\u001b[1mGold Response: \u001b[0m\n",
      "no . my camera is n't good enough to get good pictures , they are so small END_DECODE\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "\u001b[1mPast response: \u001b[0m\n",
      "that s UNKNOWN_INDEX ! do y all have any plans ?\n",
      "\u001b[1mPOSITIVE model Response: \u001b[0m\n",
      "START_DECODE it 's been so UNKNOWN_INDEX END_DECODE\n",
      "\u001b[1mNEGATIVE model Response: \u001b[0m\n",
      "START_DECODE i do n't know ! it 's a little END_DECODE\n",
      "\u001b[1mGold Response: \u001b[0m\n",
      "we are just going to hang out on the farm and hang out with the UNKNOWN_INDEX , ha END_DECODE\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "\u001b[1mPast response: \u001b[0m\n",
      "UNKNOWN_INDEX ! UNKNOWN_INDEX did you have a good time ?\n",
      "\u001b[1mPOSITIVE model Response: \u001b[0m\n",
      "START_DECODE UNKNOWN_INDEX i have no ! we are a UNKNOWN_INDEX END_DECODE\n",
      "\u001b[1mNEGATIVE model Response: \u001b[0m\n",
      "START_DECODE it did not . they are just going out END_DECODE\n",
      "\u001b[1mGold Response: \u001b[0m\n",
      "i did , thanks for asking . i went with my older brother and a few of his friends . it was a blast END_DECODE\n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_batched_past_turn_idx, dev_batched_past_lengths, dev_batched_response_idx, dev_batched_resp_lengths, dev_batched_emotions= dev_dataset_positive.get_batches(BATCH_SIZE, vocab, emotset)\n",
    "for _ in range(5):\n",
    "  rand_batch = random.randint(0, len(dev_batched_past_turn_idx)-1)\n",
    "  rand_item = random.randint(0, len(dev_batched_past_turn_idx[0])-1)\n",
    "  print(f\"{colors.BOLD}Past response: {colors.END}\")\n",
    "  s = id2string(vocab, [x for x in dev_batched_past_turn_idx[rand_batch][rand_item] if x != PAD_INDEX])\n",
    "  print(' '.join(s))\n",
    "  print(f\"{colors.BOLD}POSITIVE model Response: {colors.END}\")\n",
    "  model_resp = decode(model=positive_model, prev_turn=[dev_batched_past_turn_idx[rand_batch][rand_item]], prev_lengths=[dev_batched_past_lengths[rand_batch][rand_item]], vocab=vocab, max_len=20, random_top_k=True)\n",
    "  print(' '.join(model_resp))\n",
    "  print(f\"{colors.BOLD}NEGATIVE model Response: {colors.END}\")\n",
    "  model_resp = decode(model=negative_model, prev_turn=[dev_batched_past_turn_idx[rand_batch][rand_item]], prev_lengths=[dev_batched_past_lengths[rand_batch][rand_item]], vocab=vocab, max_len=20, random_top_k=True)\n",
    "  print(' '.join(model_resp))\n",
    "  print(f\"{colors.BOLD}Gold Response: {colors.END}\")\n",
    "  s = id2string(vocab, [x for x in dev_batched_response_idx[rand_batch][rand_item][1:] if x != PAD_INDEX])\n",
    "  print(' '.join(s))\n",
    "  print()\n",
    "  print(\"---------------------------------\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjUfvnSwzEPb"
   },
   "source": [
    "### Let's decode *both* models on some *negative* data to see the types of responses it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  model_resp = decode(model=positive_model, prev_turn=[dev_batched_past_turn_idx[rand_batch][rand_item]], prev_lengths=[dev_batched_past_lengths[rand_batch][rand_item]], vocab=vocab, max_len=20, random_top_k=True)\n",
    "\n",
    "\n",
    "  model_resp = decode(model=negative_model, prev_turn=[dev_batched_past_turn_idx[rand_batch][rand_item]], prev_lengths=[dev_batched_past_lengths[rand_batch][rand_item]], vocab=vocab, max_len=20, random_top_k=True)\n",
    "  print(' '.join(model_resp))\n",
    "\n",
    "  s = id2string(vocab, [x for x in dev_batched_response_idx[rand_batch][rand_item][1:] if x != PAD_INDEX])\n",
    "  print(' '.join(s))\n",
    "  print()\n",
    "  print(\"---------------------------------\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ccSl8SkyveX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPast response: \u001b[0m\n",
      "i hate it when that happens ! were you ok ?\n",
      "\u001b[1mPOSITIVE model Response: \u001b[0m\n",
      "START_DECODE UNKNOWN_INDEX ! UNKNOWN_INDEX they are going back END_DECODE\n",
      "\u001b[1mNEGATIVE model Response: \u001b[0m\n",
      "START_DECODE yeah ! ! UNKNOWN_INDEX UNKNOWN_INDEX UNKNOWN_INDEX . UNKNOWN_INDEX was a good END_DECODE\n",
      "\u001b[1mGold Response: \u001b[0m\n",
      "i made it out ok , just so END_DECODE\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "\u001b[1mPast response: \u001b[0m\n",
      "hopefully you do n't get in trouble but i definitely know the feeling .\n",
      "\u001b[1mPOSITIVE model Response: \u001b[0m\n",
      "START_DECODE thanks END_DECODE\n",
      "\u001b[1mNEGATIVE model Response: \u001b[0m\n",
      "START_DECODE yes it , so UNKNOWN_INDEX UNKNOWN_INDEX . UNKNOWN_INDEX is so i 'm not sure what they are END_DECODE\n",
      "\u001b[1mGold Response: \u001b[0m\n",
      "i UNKNOWN_INDEX UNKNOWN_INDEX the meeting a little bit , but probably not as much as if i 'd gone to it END_DECODE\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "\u001b[1mPast response: \u001b[0m\n",
      "oh what a mess . i would hope that they picked it up .\n",
      "\u001b[1mPOSITIVE model Response: \u001b[0m\n",
      "START_DECODE i am going out to see them with the best time and it . UNKNOWN_INDEX it END_DECODE\n",
      "\u001b[1mNEGATIVE model Response: \u001b[0m\n",
      "START_DECODE i have no one i have never been END_DECODE\n",
      "\u001b[1mGold Response: \u001b[0m\n",
      "they did as soon as they got home and saw END_DECODE\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "\u001b[1mPast response: \u001b[0m\n",
      "oh no , i am sorry to hear that . that really UNKNOWN_INDEX , maybe you can UNKNOWN_INDEX soon ?\n",
      "\u001b[1mPOSITIVE model Response: \u001b[0m\n",
      "START_DECODE i 'm sure ! i have a good UNKNOWN_INDEX of them END_DECODE\n",
      "\u001b[1mNEGATIVE model Response: \u001b[0m\n",
      "START_DECODE it 's just just the time END_DECODE\n",
      "\u001b[1mGold Response: \u001b[0m\n",
      "i UNKNOWN_INDEX i could , but i 'm not sure i want to apply again as they did n't treat with any respect , and ignored my END_DECODE\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "\u001b[1mPast response: \u001b[0m\n",
      "UNKNOWN_INDEX . UNKNOWN_INDEX be careful UNKNOWN_INDEX that s a ton of UNKNOWN_INDEX .\n",
      "\u001b[1mPOSITIVE model Response: \u001b[0m\n",
      "START_DECODE yes ! we 've never had been been a few times and i 'm glad you have never n't know\n",
      "\u001b[1mNEGATIVE model Response: \u001b[0m\n",
      "START_DECODE UNKNOWN_INDEX , but i am UNKNOWN_INDEX . UNKNOWN_INDEX i do nt get a few people do it . it 's\n",
      "\u001b[1mGold Response: \u001b[0m\n",
      "it 's better then the alternative . one good meal then water and UNKNOWN_INDEX soup the rest of the week END_DECODE\n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_batched_past_turn_idx, dev_batched_past_lengths, dev_batched_response_idx, dev_batched_resp_lengths, dev_batched_emotions= dev_dataset_negative.get_batches(BATCH_SIZE, vocab, emotset)\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "  rand_batch = random.randint(0, len(dev_batched_past_turn_idx)-1)\n",
    "  rand_item = random.randint(0, len(dev_batched_past_turn_idx[0])-1)\n",
    "  print(f\"{colors.BOLD}Past response: {colors.END}\")\n",
    "  s = id2string(vocab, [x for x in dev_batched_past_turn_idx[rand_batch][rand_item] if x != PAD_INDEX])\n",
    "  print(' '.join(s))\n",
    "  print(f\"{colors.BOLD}POSITIVE model Response: {colors.END}\")\n",
    "  model_resp = decode(model=positive_model, prev_turn=[dev_batched_past_turn_idx[rand_batch][rand_item]], prev_lengths=[dev_batched_past_lengths[rand_batch][rand_item]], vocab=vocab, max_len=20, random_top_k=True)\n",
    "  print(' '.join(model_resp))\n",
    "  print(f\"{colors.BOLD}NEGATIVE model Response: {colors.END}\")\n",
    "  model_resp = decode(model=negative_model, prev_turn=[dev_batched_past_turn_idx[rand_batch][rand_item]], prev_lengths=[dev_batched_past_lengths[rand_batch][rand_item]], vocab=vocab, max_len=20, random_top_k=True)\n",
    "  print(' '.join(model_resp))\n",
    "  print(f\"{colors.BOLD}Gold Response: {colors.END}\")\n",
    "  s = id2string(vocab, [x for x in dev_batched_response_idx[rand_batch][rand_item][1:] if x != PAD_INDEX])\n",
    "  print(' '.join(s))\n",
    "  print()\n",
    "  print(\"---------------------------------\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2Gy5akr0kTq"
   },
   "source": [
    "### Now, let's see which model does better when evaluated on the other's development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSnaRk490oq9"
   },
   "outputs": [],
   "source": [
    "def compare_positive_and_negative_data(positive_model, negative_model, dev_dataset_positive, dev_dataset_negative):\n",
    "  '''\n",
    "  This method compares the positive model on the negative dataset and the negative model on the positive dataset.\n",
    "  You should use evaluate_on_data to get the training loss for each model.\n",
    "\n",
    "  Arguments:\n",
    "  positive_model:       model trained on positive data\n",
    "  negative_model:       model trained on negative data\n",
    "  dev_dataset_positive: dev dataset for positive data\n",
    "  dev_dataset_negative: dev dataset for negative data\n",
    "\n",
    "  Returns:\n",
    "  positive_model_negative_data:   result of evaluating the positive model on the negative dev dataset\n",
    "  negative_model_positive_data:   result of evaluating the negative model on the positive dev dataset\n",
    "  '''\n",
    "\n",
    "  batch_size = 512\n",
    "  positive_model_negative_data = evaluate_on_data(positive_model, dev_dataset_negative, batch_size, vocab,  emotset)\n",
    "  negative_model_positive_data = evaluate_on_data(negative_model, dev_dataset_positive, batch_size, vocab,  emotset)\n",
    "  return positive_model_negative_data, negative_model_positive_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBORbA8oD2Xc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive model loss on negative data: 4.599264621734619\n",
      "Negative model loss on positive data: 4.785733699798584\n"
     ]
    }
   ],
   "source": [
    "positive_model_negative_data, negative_model_positive_data = compare_positive_and_negative_data(positive_model,  \n",
    "                                                                                                negative_model, \n",
    "                                                                                                dev_dataset_positive, \n",
    "                                                                                                dev_dataset_negative)\n",
    "print(\"Positive model loss on negative data:\", positive_model_negative_data)\n",
    "print(\"Negative model loss on positive data:\", negative_model_positive_data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "HW_7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
